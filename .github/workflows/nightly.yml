name: Nightly Verification

on:
  schedule:
    # Run at 3 AM UTC every night
    - cron: '0 3 * * *'
  workflow_dispatch:

jobs:
  comprehensive-verification:
    name: Comprehensive Nightly Verification
    runs-on: ubuntu-latest
    timeout-minutes: 240
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Setup environment
        run: |
          echo "Setting up verification environment..."
          mkdir -p results
          mkdir -p reports
      
      - name: Setup Java
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '17'
      
      - name: Cache TLA+ tools
        uses: actions/cache@v3
        with:
          path: ~/tla-tools
          key: tla-tools-1.8.0
      
      - name: Install TLA+ tools
        run: |
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            mkdir -p ~/tla-tools
            cd ~/tla-tools
            wget -q https://github.com/tlaplus/tlaplus/releases/download/v1.8.0/tla2tools.jar
          fi
      
      - name: Run comprehensive verification
        run: |
          echo "Starting comprehensive nightly verification..."
          
          # Run all configurations
          for config in Small Medium Stress; do
            echo "Running $config configuration..."
            java -Xmx16G -cp ~/tla-tools/tla2tools.jar tlc2.TLC \
              -config models/$config.cfg \
              -workers auto \
              -cleanup \
              -deadlock \
              -coverage 1 \
              specs/Alpenglow.tla > results/${config}_$(date +%Y%m%d).log 2>&1 || true
          done
      
      - name: Generate performance metrics
        run: |
          echo "# Nightly Performance Report" >> $GITHUB_STEP_SUMMARY
          echo "Date: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          for log in results/*.log; do
            if [ -f "$log" ]; then
              CONFIG=$(basename "$log" | cut -d_ -f1)
              echo "## $CONFIG Configuration" >> $GITHUB_STEP_SUMMARY
              
              STATES=$(grep "states generated" "$log" | tail -1 | grep -oE '[0-9]+' | head -1 || echo "0")
              echo "- States: $STATES" >> $GITHUB_STEP_SUMMARY
              
              RUNTIME=$(grep "Elapsed time" "$log" | tail -1 || echo "N/A")
              echo "- Runtime: $RUNTIME" >> $GITHUB_STEP_SUMMARY
              
              if grep -q "No error" "$log"; then
                echo "- Result: ✅ Passed" >> $GITHUB_STEP_SUMMARY
              else
                echo "- Result: ❌ Failed" >> $GITHUB_STEP_SUMMARY
              fi
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          done
      
      - name: Compare with baseline
        run: |
          echo "## Performance Comparison" >> $GITHUB_STEP_SUMMARY
          echo "Comparing with previous runs..." >> $GITHUB_STEP_SUMMARY
          # In a real setup, this would compare with historical data
          echo "⚠️ Historical comparison not yet implemented" >> $GITHUB_STEP_SUMMARY
      
      - name: Upload nightly results
        uses: actions/upload-artifact@v3
        with:
          name: nightly-results-${{ github.run_number }}
          path: results/
          retention-days: 30
      
      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            const title = `Nightly Verification Failed - ${new Date().toISOString().split('T')[0]}`;
            const body = `The nightly verification run has failed.
            
            **Run ID:** ${{ github.run_id }}
            **Run Number:** ${{ github.run_number }}
            **Commit:** ${{ github.sha }}
            
            Please review the [workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.`;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['verification-failure', 'nightly']
            });

  regression-testing:
    name: Regression Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Setup Java
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '17'
      
      - name: Cache TLA+ tools
        uses: actions/cache@v3
        with:
          path: ~/tla-tools
          key: tla-tools-1.8.0
      
      - name: Run regression tests
        run: |
          echo "Running regression test suite..."
          
          # Test specific scenarios that have previously failed
          SCENARIOS=(
            "byzantine_leader"
            "network_partition"
            "cascading_failures"
            "message_reordering"
          )
          
          for scenario in "${SCENARIOS[@]}"; do
            echo "Testing scenario: $scenario"
            # In a real setup, this would run specific test configurations
            echo "✅ $scenario test placeholder"
          done
      
      - name: Generate regression report
        run: |
          echo "# Regression Test Report" >> $GITHUB_STEP_SUMMARY
          echo "All regression tests completed" >> $GITHUB_STEP_SUMMARY

  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Setup Java
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '17'
      
      - name: Cache TLA+ tools
        uses: actions/cache@v3
        with:
          path: ~/tla-tools
          key: tla-tools-1.8.0
      
      - name: Run performance benchmarks
        run: |
          echo "Running performance benchmarks..."
          
          # Measure verification speed for different validator counts
          for validators in 5 10 20 30; do
            echo "Benchmarking with $validators validators..."
            START=$(date +%s)
            
            # Create temporary config
            cat > /tmp/bench_${validators}.cfg << EOF
          SPECIFICATION Spec
          CONSTANTS
              NumValidators = $validators
              MaxSlot = 10
              ByzantineValidators = {}
              OfflineValidators = {}
          INVARIANTS
              TypeInvariant
              SafetyInvariant
          EOF
            
            timeout 300 java -Xmx8G -cp ~/tla-tools/tla2tools.jar tlc2.TLC \
              -config /tmp/bench_${validators}.cfg \
              -workers 4 \
              specs/Alpenglow.tla > /tmp/bench_${validators}.log 2>&1 || true
            
            END=$(date +%s)
            DURATION=$((END - START))
            
            echo "Validators: $validators, Time: ${DURATION}s" >> benchmark_results.txt
          done
      
      - name: Plot benchmark results
        run: |
          echo "## Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat benchmark_results.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark_results.txt

  notification:
    name: Send Summary
    needs: [comprehensive-verification, regression-testing, benchmark]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Generate summary
        run: |
          echo "# Nightly Verification Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "- Comprehensive Verification: ${{ needs.comprehensive-verification.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Regression Testing: ${{ needs.regression-testing.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Performance Benchmark: ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.comprehensive-verification.result }}" != "success" ] || \
             [ "${{ needs.regression-testing.result }}" != "success" ] || \
             [ "${{ needs.benchmark.result }}" != "success" ]; then
            echo "⚠️ **Some jobs failed. Please review the logs.**" >> $GITHUB_STEP_SUMMARY
          else
            echo "✅ **All nightly verification jobs completed successfully!**" >> $GITHUB_STEP_SUMMARY
          fi
