//! Common test utilities module for Alpenglow Stateright verification
//!
//! This module provides shared functionality for all test scenarios including:
//! - CLI argument parsing
//! - JSON configuration loading and validation
//! - Test report generation and output
//! - Configuration conversion utilities
//! - Error handling and logging setup
//!
//! All test scenario binaries import this module to provide consistent CLI integration
//! with the verification script.

use std::collections::BTreeMap;
use std::path::Path;
use std::time::{Duration, Instant};
use std::fs;
use std::io::Write;

use serde::{Deserialize, Serialize};
use clap::{Parser, ValueEnum};
use anyhow::{Context, Result as AnyhowResult};
use chrono::{DateTime, Utc};

// Import core types from the main library
use crate::{Config, ValidatorId, StakeAmount, TimeValue};

/// CLI arguments structure for all test scenario binaries
#[derive(Parser, Debug, Clone)]
#[command(author, version, about, long_about = None)]
pub struct CliArgs {
    /// Path to JSON configuration file
    #[arg(long, value_name = "FILE")]
    pub config: String,

    /// Path to output JSON results file
    #[arg(long, value_name = "FILE")]
    pub output: String,

    /// Enable verbose logging
    #[arg(short, long)]
    pub verbose: bool,

    /// Timeout for verification in seconds
    #[arg(long, default_value = "300")]
    pub timeout: u64,

    /// Maximum exploration depth
    #[arg(long, default_value = "1000")]
    pub depth: usize,

    /// Enable cross-validation with TLA+
    #[arg(long)]
    pub cross_validate: bool,

    /// Enable external Stateright verification
    #[arg(long)]
    pub external_stateright: bool,
}

/// Test configuration structure that mirrors the JSON config generated by the verification script
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestConfig {
    /// Number of validators in the test network
    pub validators: usize,

    /// Number of Byzantine validators to simulate
    pub byzantine_count: usize,

    /// Number of offline validators to simulate
    pub offline_count: usize,

    /// Maximum number of consensus rounds to explore
    pub max_rounds: usize,

    /// Network delay in milliseconds
    pub network_delay: u64,

    /// Timeout for consensus operations in milliseconds
    pub timeout_ms: u64,

    /// Maximum exploration depth for model checking
    pub exploration_depth: usize,

    /// Leader window size for VRF leader selection
    #[serde(default = "default_leader_window_size")]
    pub leader_window_size: usize,

    /// Enable adaptive timeout mechanisms
    #[serde(default = "default_true")]
    pub adaptive_timeouts: bool,

    /// Enable VRF-based leader selection
    #[serde(default = "default_true")]
    pub vrf_enabled: bool,

    /// Enable network partition testing
    #[serde(default)]
    pub network_partitions: bool,

    /// Enable stress testing mode
    #[serde(default)]
    pub stress_test: bool,

    /// Enable edge case testing
    #[serde(default)]
    pub test_edge_cases: bool,

    /// Custom stake distribution (optional)
    #[serde(default)]
    pub stake_distribution: Option<BTreeMap<ValidatorId, StakeAmount>>,

    /// Bandwidth limit per validator in bytes/second
    #[serde(default = "default_bandwidth_limit")]
    pub bandwidth_limit: u64,

    /// Erasure coding parameters (k, n)
    #[serde(default)]
    pub erasure_coding: Option<(usize, usize)>,

    /// Fast path threshold as percentage of total stake
    #[serde(default = "default_fast_path_threshold")]
    pub fast_path_threshold: f64,

    /// Slow path threshold as percentage of total stake
    #[serde(default = "default_slow_path_threshold")]
    pub slow_path_threshold: f64,
}

/// Test report structure for JSON output with comprehensive metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestReport {
    /// Test scenario name
    pub scenario: String,

    /// Number of states explored during verification
    pub states_explored: usize,

    /// Number of properties checked
    pub properties_checked: usize,

    /// Number of property violations found
    pub violations: usize,

    /// Test duration in milliseconds
    pub duration_ms: u64,

    /// Overall test success status
    pub success: bool,

    /// Detailed results for each property
    pub property_results: Vec<PropertyResult>,

    /// Performance metrics
    pub metrics: TestMetrics,

    /// Configuration used for the test
    pub config: TestConfig,

    /// Timestamp when test was run
    pub timestamp: DateTime<Utc>,

    /// Additional metadata
    pub metadata: TestMetadata,
}

/// Result for an individual property check
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PropertyResult {
    /// Property name
    pub name: String,

    /// Whether the property passed
    pub passed: bool,

    /// States explored for this property
    pub states_explored: usize,

    /// Duration for this property check in milliseconds
    pub duration_ms: u64,

    /// Error message if property failed
    pub error: Option<String>,

    /// Counterexample length if property failed
    pub counterexample_length: Option<usize>,

    /// Cross-validation results
    pub cross_validation: Option<CrossValidationResult>,
}

/// Cross-validation results from different verification approaches
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrossValidationResult {
    /// Result from local Stateright implementation
    pub local_result: bool,

    /// Result from external Stateright framework
    pub external_result: Option<bool>,

    /// Result from TLA+ model checker
    pub tla_result: Option<bool>,

    /// Whether all approaches agree
    pub consistent: bool,

    /// Details about any inconsistencies
    pub details: String,
}

/// Performance and resource usage metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestMetrics {
    /// Peak memory usage in bytes
    pub peak_memory_bytes: u64,

    /// Average states explored per second
    pub states_per_second: f64,

    /// Total CPU time used in milliseconds
    pub cpu_time_ms: u64,

    /// Number of timeouts encountered
    pub timeouts: usize,

    /// Number of Byzantine behaviors detected
    pub byzantine_events: usize,

    /// Number of network events processed
    pub network_events: usize,

    /// Coverage metrics
    pub coverage: CoverageMetrics,
}

/// Code and state space coverage metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CoverageMetrics {
    /// Percentage of state space explored
    pub state_space_coverage: f64,

    /// Number of unique states visited
    pub unique_states: usize,

    /// Number of transitions explored
    pub transitions: usize,

    /// Percentage of code paths exercised
    pub code_coverage: f64,
}

/// Additional test metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestMetadata {
    /// Rust version used
    pub rust_version: String,

    /// Stateright version used
    pub stateright_version: String,

    /// Host system information
    pub hostname: String,

    /// Environment variables relevant to testing
    pub environment: BTreeMap<String, String>,

    /// Git commit hash if available
    pub git_commit: Option<String>,
}

/// Error types for test utilities
#[derive(Debug, thiserror::Error)]
pub enum TestError {
    #[error("Configuration error: {0}")]
    Config(String),

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    #[error("JSON serialization error: {0}")]
    Json(#[from] serde_json::Error),

    #[error("Verification error: {0}")]
    Verification(String),

    #[error("Timeout error: {0}")]
    Timeout(String),

    #[error("Cross-validation error: {0}")]
    CrossValidation(String),
}

// Default value functions for serde
fn default_leader_window_size() -> usize { 4 }
fn default_true() -> bool { true }
fn default_bandwidth_limit() -> u64 { 1_000_000 } // 1 MB/s
fn default_fast_path_threshold() -> f64 { 0.67 } // 2/3 + 1
fn default_slow_path_threshold() -> f64 { 0.51 } // 1/2 + 1

impl Default for TestConfig {
    fn default() -> Self {
        Self {
            validators: 4,
            byzantine_count: 1,
            offline_count: 0,
            max_rounds: 10,
            network_delay: 100,
            timeout_ms: 5000,
            exploration_depth: 1000,
            leader_window_size: default_leader_window_size(),
            adaptive_timeouts: default_true(),
            vrf_enabled: default_true(),
            network_partitions: false,
            stress_test: false,
            test_edge_cases: false,
            stake_distribution: None,
            bandwidth_limit: default_bandwidth_limit(),
            erasure_coding: None,
            fast_path_threshold: default_fast_path_threshold(),
            slow_path_threshold: default_slow_path_threshold(),
        }
    }
}

/// Parse CLI arguments using clap
pub fn parse_cli_args() -> CliArgs {
    CliArgs::parse()
}

/// Load test configuration from JSON file
pub fn load_config(path: &Path) -> Result<TestConfig, TestError> {
    let content = fs::read_to_string(path)
        .with_context(|| format!("Failed to read config file: {}", path.display()))?;
    
    let config: TestConfig = serde_json::from_str(&content)
        .with_context(|| format!("Failed to parse config file: {}", path.display()))?;
    
    // Validate configuration
    validate_config(&config)?;
    
    Ok(config)
}

/// Validate test configuration for consistency and safety
fn validate_config(config: &TestConfig) -> Result<(), TestError> {
    if config.validators == 0 {
        return Err(TestError::Config("Must have at least 1 validator".to_string()));
    }

    if config.byzantine_count >= config.validators {
        return Err(TestError::Config(
            "Byzantine count must be less than total validators".to_string()
        ));
    }

    if config.offline_count >= config.validators {
        return Err(TestError::Config(
            "Offline count must be less than total validators".to_string()
        ));
    }

    if config.byzantine_count + config.offline_count >= config.validators {
        return Err(TestError::Config(
            "Byzantine + offline count must be less than total validators".to_string()
        ));
    }

    // Check Byzantine fault tolerance threshold (< 1/3)
    if config.byzantine_count * 3 >= config.validators {
        return Err(TestError::Config(
            "Byzantine count must be less than 1/3 of total validators for safety".to_string()
        ));
    }

    if config.exploration_depth == 0 {
        return Err(TestError::Config("Exploration depth must be greater than 0".to_string()));
    }

    if config.timeout_ms == 0 {
        return Err(TestError::Config("Timeout must be greater than 0".to_string()));
    }

    // Validate stake distribution if provided
    if let Some(ref stakes) = config.stake_distribution {
        if stakes.len() != config.validators {
            return Err(TestError::Config(
                "Stake distribution must include all validators".to_string()
            ));
        }

        let total_stake: StakeAmount = stakes.values().sum();
        if total_stake == 0 {
            return Err(TestError::Config("Total stake must be greater than 0".to_string()));
        }
    }

    // Validate erasure coding parameters
    if let Some((k, n)) = config.erasure_coding {
        if k == 0 || n == 0 {
            return Err(TestError::Config("Erasure coding parameters must be greater than 0".to_string()));
        }
        if k > n {
            return Err(TestError::Config("Erasure coding k must be <= n".to_string()));
        }
    }

    // Validate threshold percentages
    if config.fast_path_threshold <= 0.0 || config.fast_path_threshold > 1.0 {
        return Err(TestError::Config("Fast path threshold must be between 0 and 1".to_string()));
    }

    if config.slow_path_threshold <= 0.0 || config.slow_path_threshold > 1.0 {
        return Err(TestError::Config("Slow path threshold must be between 0 and 1".to_string()));
    }

    Ok(())
}

/// Convert test configuration to protocol configuration
pub fn convert_to_alpenglow_config(test_config: &TestConfig) -> Config {
    let mut config = Config::new()
        .with_validators(test_config.validators)
        .with_byzantine_threshold(test_config.byzantine_count);

    // Set network timing parameters
    config = config.with_network_timing(
        test_config.network_delay,
        test_config.timeout_ms
    );

    // Set stake distribution if provided
    if let Some(ref stakes) = test_config.stake_distribution {
        config = config.with_stake_distribution(stakes.clone());
    }

    // Set erasure coding parameters if provided
    if let Some((k, n)) = test_config.erasure_coding {
        config = config.with_erasure_coding(k, n);
    }

    // Set bandwidth limit
    config.bandwidth_limit = test_config.bandwidth_limit;

    // Set threshold parameters
    let total_stake: StakeAmount = config.stake_distribution.values().sum();
    config.fast_path_threshold = (test_config.fast_path_threshold * total_stake as f64) as StakeAmount;
    config.slow_path_threshold = (test_config.slow_path_threshold * total_stake as f64) as StakeAmount;

    // Set additional test-specific parameters
    config.leader_window_size = test_config.leader_window_size;
    config.adaptive_timeouts = test_config.adaptive_timeouts;
    config.vrf_enabled = test_config.vrf_enabled;

    config
}

/// Emit test report to JSON file
pub fn emit_report(report: &TestReport, path: &Path) -> Result<(), TestError> {
    let json = serde_json::to_string_pretty(report)
        .context("Failed to serialize test report")?;
    
    let mut file = fs::File::create(path)
        .with_context(|| format!("Failed to create output file: {}", path.display()))?;
    
    file.write_all(json.as_bytes())
        .with_context(|| format!("Failed to write to output file: {}", path.display()))?;
    
    file.flush()
        .with_context(|| format!("Failed to flush output file: {}", path.display()))?;
    
    Ok(())
}

/// Create a new test report with basic information
pub fn create_test_report(scenario: &str, config: TestConfig) -> TestReport {
    TestReport {
        scenario: scenario.to_string(),
        states_explored: 0,
        properties_checked: 0,
        violations: 0,
        duration_ms: 0,
        success: false,
        property_results: Vec::new(),
        metrics: TestMetrics {
            peak_memory_bytes: 0,
            states_per_second: 0.0,
            cpu_time_ms: 0,
            timeouts: 0,
            byzantine_events: 0,
            network_events: 0,
            coverage: CoverageMetrics {
                state_space_coverage: 0.0,
                unique_states: 0,
                transitions: 0,
                code_coverage: 0.0,
            },
        },
        config,
        timestamp: Utc::now(),
        metadata: create_test_metadata(),
    }
}

/// Create test metadata with system information
fn create_test_metadata() -> TestMetadata {
    let mut environment = BTreeMap::new();
    
    // Collect relevant environment variables
    for (key, value) in std::env::vars() {
        if key.starts_with("ALPENGLOW_") || 
           key.starts_with("STATERIGHT_") ||
           key.starts_with("RUST_") ||
           key == "CI" ||
           key == "GITHUB_ACTIONS" {
            environment.insert(key, value);
        }
    }

    TestMetadata {
        rust_version: get_rust_version(),
        stateright_version: get_stateright_version(),
        hostname: get_hostname(),
        environment,
        git_commit: get_git_commit(),
    }
}

/// Get Rust version information
fn get_rust_version() -> String {
    std::env::var("RUSTC_VERSION")
        .or_else(|_| {
            std::process::Command::new("rustc")
                .arg("--version")
                .output()
                .ok()
                .and_then(|output| String::from_utf8(output.stdout).ok())
                .map(|s| s.trim().to_string())
        })
        .unwrap_or_else(|| "unknown".to_string())
}

/// Get Stateright version information
fn get_stateright_version() -> String {
    // This would ideally come from the Stateright crate version
    env!("CARGO_PKG_VERSION").to_string()
}

/// Get hostname
fn get_hostname() -> String {
    std::env::var("HOSTNAME")
        .or_else(|_| std::env::var("COMPUTERNAME"))
        .or_else(|_| {
            std::process::Command::new("hostname")
                .output()
                .ok()
                .and_then(|output| String::from_utf8(output.stdout).ok())
                .map(|s| s.trim().to_string())
        })
        .unwrap_or_else(|| "unknown".to_string())
}

/// Get git commit hash if available
fn get_git_commit() -> Option<String> {
    std::process::Command::new("git")
        .args(&["rev-parse", "HEAD"])
        .output()
        .ok()
        .and_then(|output| {
            if output.status.success() {
                String::from_utf8(output.stdout).ok()
                    .map(|s| s.trim().to_string())
            } else {
                None
            }
        })
}

/// Setup logging based on verbosity level
pub fn setup_logging(verbose: bool) -> Result<(), TestError> {
    let level = if verbose {
        tracing::Level::DEBUG
    } else {
        tracing::Level::INFO
    };

    tracing_subscriber::fmt()
        .with_max_level(level)
        .with_target(false)
        .with_thread_ids(false)
        .with_file(false)
        .with_line_number(false)
        .init();

    Ok(())
}

/// Measure execution time and update report
pub fn measure_execution<F, R>(f: F) -> (R, Duration)
where
    F: FnOnce() -> R,
{
    let start = Instant::now();
    let result = f();
    let duration = start.elapsed();
    (result, duration)
}

/// Add property result to test report
pub fn add_property_result(
    report: &mut TestReport,
    name: String,
    passed: bool,
    states_explored: usize,
    duration: Duration,
    error: Option<String>,
    counterexample_length: Option<usize>,
) {
    let property_result = PropertyResult {
        name,
        passed,
        states_explored,
        duration_ms: duration.as_millis() as u64,
        error,
        counterexample_length,
        cross_validation: None,
    };

    report.property_results.push(property_result);
    report.properties_checked += 1;
    report.states_explored += states_explored;
    
    if !passed {
        report.violations += 1;
    }
}

/// Update test report with final results
pub fn finalize_report(report: &mut TestReport, total_duration: Duration) {
    report.duration_ms = total_duration.as_millis() as u64;
    report.success = report.violations == 0;
    
    // Calculate performance metrics
    if report.duration_ms > 0 {
        report.metrics.states_per_second = 
            (report.states_explored as f64) / (report.duration_ms as f64 / 1000.0);
    }
    
    // Update coverage metrics
    report.metrics.coverage.unique_states = report.states_explored;
    report.metrics.coverage.transitions = report.states_explored.saturating_sub(1);
    
    // Calculate state space coverage (simplified estimate)
    let theoretical_max_states = 2_usize.pow(report.config.validators as u32 * 10); // Rough estimate
    report.metrics.coverage.state_space_coverage = 
        (report.states_explored as f64) / (theoretical_max_states as f64).min(1_000_000.0);
}

/// Create a cross-validation result
pub fn create_cross_validation_result(
    local_result: bool,
    external_result: Option<bool>,
    tla_result: Option<bool>,
) -> CrossValidationResult {
    let mut consistent = true;
    let mut details = String::new();

    if let Some(external) = external_result {
        if local_result != external {
            consistent = false;
            details.push_str("Local/External Stateright mismatch; ");
        }
    }

    if let Some(tla) = tla_result {
        if local_result != tla {
            consistent = false;
            details.push_str("Local/TLA+ mismatch; ");
        }
    }

    if consistent {
        details = "All verification approaches agree".to_string();
    }

    CrossValidationResult {
        local_result,
        external_result,
        tla_result,
        consistent,
        details,
    }
}

/// Utility function to check if running in CI environment
pub fn is_ci_environment() -> bool {
    std::env::var("CI").is_ok() || 
    std::env::var("GITHUB_ACTIONS").is_ok() ||
    std::env::var("GITLAB_CI").is_ok() ||
    std::env::var("TRAVIS").is_ok()
}

/// Get timeout duration from config and CLI args
pub fn get_timeout_duration(config: &TestConfig, cli_timeout: u64) -> Duration {
    // Use CLI timeout if specified, otherwise use config timeout
    let timeout_ms = if cli_timeout > 0 {
        cli_timeout * 1000 // Convert seconds to milliseconds
    } else {
        config.timeout_ms
    };
    
    Duration::from_millis(timeout_ms)
}

/// Print test summary to console
pub fn print_test_summary(report: &TestReport) {
    println!("\n=== {} Test Summary ===", report.scenario);
    println!("Success: {}", if report.success { "✓" } else { "✗" });
    println!("Properties checked: {}", report.properties_checked);
    println!("Violations found: {}", report.violations);
    println!("States explored: {}", report.states_explored);
    println!("Duration: {}ms", report.duration_ms);
    println!("Performance: {:.1} states/sec", report.metrics.states_per_second);
    
    if !report.property_results.is_empty() {
        println!("\nProperty Results:");
        for result in &report.property_results {
            let status = if result.passed { "✓" } else { "✗" };
            println!("  {} {}: {}ms", status, result.name, result.duration_ms);
            if let Some(ref error) = result.error {
                println!("    Error: {}", error);
            }
        }
    }
    
    println!("Report saved to output file");
}

/// Validate that output directory exists and is writable
pub fn validate_output_path(path: &Path) -> Result<(), TestError> {
    if let Some(parent) = path.parent() {
        if !parent.exists() {
            fs::create_dir_all(parent)
                .with_context(|| format!("Failed to create output directory: {}", parent.display()))?;
        }
    }
    
    // Test write permissions by creating a temporary file
    let temp_path = path.with_extension("tmp");
    fs::File::create(&temp_path)
        .and_then(|_| fs::remove_file(&temp_path))
        .with_context(|| format!("Output path is not writable: {}", path.display()))?;
    
    Ok(())
}

/// Common main function template for test binaries
pub fn run_test_scenario<F>(
    scenario_name: &str,
    verification_fn: F,
) -> Result<(), Box<dyn std::error::Error>>
where
    F: FnOnce(&Config, &TestConfig) -> Result<TestReport, TestError>,
{
    // Parse CLI arguments
    let args = parse_cli_args();
    
    // Setup logging
    setup_logging(args.verbose)?;
    
    // Load configuration
    let config_path = Path::new(&args.config);
    let test_config = load_config(config_path)?;
    
    // Validate output path
    let output_path = Path::new(&args.output);
    validate_output_path(output_path)?;
    
    // Convert to protocol config
    let protocol_config = convert_to_alpenglow_config(&test_config);
    
    // Run verification
    let (mut report, duration) = measure_execution(|| {
        verification_fn(&protocol_config, &test_config)
    });
    
    match report {
        Ok(mut report) => {
            // Finalize report with timing information
            finalize_report(&mut report, duration);
            
            // Print summary
            print_test_summary(&report);
            
            // Emit report
            emit_report(&report, output_path)?;
            
            // Exit with appropriate code
            std::process::exit(if report.success { 0 } else { 1 });
        }
        Err(e) => {
            // Create error report
            let mut error_report = create_test_report(scenario_name, test_config);
            error_report.success = false;
            error_report.violations = 1;
            finalize_report(&mut error_report, duration);
            
            // Add error details
            add_property_result(
                &mut error_report,
                "Verification".to_string(),
                false,
                0,
                duration,
                Some(e.to_string()),
                None,
            );
            
            // Print error summary
            print_test_summary(&error_report);
            
            // Emit error report
            emit_report(&error_report, output_path)?;
            
            eprintln!("Verification failed: {}", e);
            std::process::exit(1);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;

    #[test]
    fn test_default_config_validation() {
        let config = TestConfig::default();
        assert!(validate_config(&config).is_ok());
    }

    #[test]
    fn test_invalid_config_validation() {
        let mut config = TestConfig::default();
        config.validators = 0;
        assert!(validate_config(&config).is_err());
        
        config.validators = 3;
        config.byzantine_count = 3;
        assert!(validate_config(&config).is_err());
    }

    #[test]
    fn test_config_conversion() {
        let test_config = TestConfig::default();
        let protocol_config = convert_to_alpenglow_config(&test_config);
        
        assert_eq!(protocol_config.validator_count, test_config.validators);
        assert_eq!(protocol_config.byzantine_threshold, test_config.byzantine_count);
    }

    #[test]
    fn test_report_creation() {
        let config = TestConfig::default();
        let report = create_test_report("test", config.clone());
        
        assert_eq!(report.scenario, "test");
        assert_eq!(report.config.validators, config.validators);
        assert!(!report.success);
    }

    #[test]
    fn test_json_serialization() {
        let config = TestConfig::default();
        let report = create_test_report("test", config);
        
        let json = serde_json::to_string(&report).unwrap();
        let deserialized: TestReport = serde_json::from_str(&json).unwrap();
        
        assert_eq!(report.scenario, deserialized.scenario);
    }

    #[test]
    fn test_file_operations() {
        let dir = tempdir().unwrap();
        let config_path = dir.path().join("config.json");
        let output_path = dir.path().join("output.json");
        
        // Test config save/load
        let config = TestConfig::default();
        let config_json = serde_json::to_string_pretty(&config).unwrap();
        fs::write(&config_path, config_json).unwrap();
        
        let loaded_config = load_config(&config_path).unwrap();
        assert_eq!(config.validators, loaded_config.validators);
        
        // Test report emission
        let report = create_test_report("test", config);
        emit_report(&report, &output_path).unwrap();
        
        assert!(output_path.exists());
    }

    #[test]
    fn test_cross_validation_result() {
        let result = create_cross_validation_result(true, Some(true), Some(true));
        assert!(result.consistent);
        
        let result = create_cross_validation_result(true, Some(false), Some(true));
        assert!(!result.consistent);
        assert!(result.details.contains("Local/External"));
    }
}