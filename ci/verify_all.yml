name: Alpenglow Protocol Verification Suite

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'specs/**'
      - 'proofs/**'
      - 'stateright/**'
      - 'models/**'
      - 'scripts/**'
      - '.github/workflows/verify_all.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'specs/**'
      - 'proofs/**'
      - 'stateright/**'
      - 'models/**'
      - 'scripts/**'
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      verification_mode:
        description: 'Verification mode'
        required: true
        default: 'full'
        type: choice
        options:
          - quick
          - full
          - stress
      enable_benchmarks:
        description: 'Run performance benchmarks'
        required: false
        default: true
        type: boolean
      cross_validate:
        description: 'Enable Stateright cross-validation'
        required: false
        default: true
        type: boolean

env:
  CARGO_TERM_COLOR: always
  TLA_TOOLS_VERSION: "1.8.0"
  TLAPS_VERSION: "1.4.5"
  RUST_BACKTRACE: 1

jobs:
  setup:
    name: Setup and Validation
    runs-on: ubuntu-latest
    outputs:
      verification-mode: ${{ steps.config.outputs.verification-mode }}
      enable-benchmarks: ${{ steps.config.outputs.enable-benchmarks }}
      cross-validate: ${{ steps.config.outputs.cross-validate }}
      matrix-configs: ${{ steps.config.outputs.matrix-configs }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure verification parameters
        id: config
        run: |
          # Determine verification mode
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            MODE="${{ github.event.inputs.verification_mode }}"
            BENCHMARKS="${{ github.event.inputs.enable_benchmarks }}"
            CROSS_VAL="${{ github.event.inputs.cross_validate }}"
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            MODE="full"
            BENCHMARKS="true"
            CROSS_VAL="true"
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            MODE="quick"
            BENCHMARKS="false"
            CROSS_VAL="true"
          else
            MODE="full"
            BENCHMARKS="true"
            CROSS_VAL="true"
          fi
          
          echo "verification-mode=$MODE" >> $GITHUB_OUTPUT
          echo "enable-benchmarks=$BENCHMARKS" >> $GITHUB_OUTPUT
          echo "cross-validate=$CROSS_VAL" >> $GITHUB_OUTPUT
          
          # Configure matrix based on mode
          if [ "$MODE" = "quick" ]; then
            CONFIGS='["Small"]'
          elif [ "$MODE" = "stress" ]; then
            CONFIGS='["Small", "Medium", "LargeScale", "Adversarial"]'
          else
            CONFIGS='["Small", "Medium", "Boundary", "EdgeCase"]'
          fi
          
          echo "matrix-configs=$CONFIGS" >> $GITHUB_OUTPUT
          
          echo "üîß Configuration:"
          echo "  Mode: $MODE"
          echo "  Benchmarks: $BENCHMARKS"
          echo "  Cross-validation: $CROSS_VAL"
          echo "  Configs: $CONFIGS"

      - name: Validate file structure
        run: |
          echo "üìÅ Validating project structure..."
          
          # Check required directories
          for dir in specs proofs models scripts; do
            if [ ! -d "$dir" ]; then
              echo "‚ùå Missing required directory: $dir"
              exit 1
            fi
          done
          
          # Check key specification files
          for spec in Alpenglow Types Network Votor Rotor; do
            if [ ! -f "specs/$spec.tla" ]; then
              echo "‚ùå Missing specification: specs/$spec.tla"
              exit 1
            fi
          done
          
          # Check proof files
          for proof in Safety Liveness; do
            if [ ! -f "proofs/$proof.tla" ]; then
              echo "‚ùå Missing proof: proofs/$proof.tla"
              exit 1
            fi
          done
          
          # Check scripts in new structure
          for script in check_model.sh verify_proofs.sh; do
            if [ ! -f "scripts/ci/$script" ]; then
              echo "‚ùå Missing CI script: scripts/ci/$script"
              exit 1
            fi
            chmod +x "scripts/ci/$script"
          done
          
          # Check for optional development scripts
          if [ -f "scripts/dev/run_all.sh" ]; then
            chmod +x "scripts/dev/run_all.sh"
          fi
          
          echo "‚úÖ Project structure validated"

  syntax-check:
    name: TLA+ Syntax Validation
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            echo "üì• Downloading TLA+ tools..."
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi
          
          # Verify installation
          java -cp ~/tla-tools/tla2tools.jar tla2sany.SANY -h > /dev/null
          echo "‚úÖ TLA+ tools installed successfully"

      - name: Validate TLA+ specifications
        run: |
          echo "üîç Validating TLA+ syntax..."
          
          SPECS=(Alpenglow Types Network Votor Rotor Integration)
          ERRORS=0
          
          for spec in "${SPECS[@]}"; do
            if [ -f "specs/$spec.tla" ]; then
              echo "  Checking $spec.tla..."
              if java -cp ~/tla-tools/tla2tools.jar tla2sany.SANY "specs/$spec.tla"; then
                echo "    ‚úÖ $spec.tla syntax valid"
              else
                echo "    ‚ùå $spec.tla has syntax errors"
                ERRORS=$((ERRORS + 1))
              fi
            else
              echo "    ‚ö†Ô∏è $spec.tla not found"
            fi
          done
          
          if [ $ERRORS -gt 0 ]; then
            echo "‚ùå $ERRORS specification(s) have syntax errors"
            exit 1
          fi
          
          echo "‚úÖ All specifications passed syntax validation"

      - name: Validate proof files
        run: |
          echo "üîç Validating proof file syntax..."
          
          PROOFS=(Safety Liveness Resilience)
          ERRORS=0
          
          for proof in "${PROOFS[@]}"; do
            if [ -f "proofs/$proof.tla" ]; then
              echo "  Checking $proof.tla..."
              if java -cp ~/tla-tools/tla2tools.jar tla2sany.SANY "proofs/$proof.tla"; then
                echo "    ‚úÖ $proof.tla syntax valid"
              else
                echo "    ‚ùå $proof.tla has syntax errors"
                ERRORS=$((ERRORS + 1))
              fi
            else
              echo "    ‚ö†Ô∏è $proof.tla not found"
            fi
          done
          
          if [ $ERRORS -gt 0 ]; then
            echo "‚ö†Ô∏è $ERRORS proof file(s) have syntax errors"
          else
            echo "‚úÖ All proof files passed syntax validation"
          fi

  model-checking:
    name: Model Checking (${{ matrix.config }})
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    strategy:
      matrix:
        config: ${{ fromJson(needs.setup.outputs.matrix-configs) }}
      fail-fast: false
    timeout-minutes: 60
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run model checking
        run: |
          echo "üîç Running model checking for ${{ matrix.config }} configuration..."
          
          # Create results directory
          mkdir -p results/ci
          
          # Set timeout based on configuration
          case "${{ matrix.config }}" in
            Small) TIMEOUT=600 ;;
            Medium) TIMEOUT=1800 ;;
            LargeScale) TIMEOUT=3600 ;;
            *) TIMEOUT=1200 ;;
          esac
          
          # Run model checking with timeout
          timeout $TIMEOUT ./scripts/ci/check_model.sh "${{ matrix.config }}" \
            > "results/ci/model_${{ matrix.config }}.log" 2>&1
          
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Model checking passed for ${{ matrix.config }}"
          elif [ $EXIT_CODE -eq 124 ]; then
            echo "‚ö†Ô∏è Model checking timed out for ${{ matrix.config }}"
            echo "timeout=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Model checking failed for ${{ matrix.config }}"
            cat "results/ci/model_${{ matrix.config }}.log"
            exit 1
          fi

      - name: Extract verification metrics
        run: |
          LOG_FILE="results/ci/model_${{ matrix.config }}.log"
          
          if [ -f "$LOG_FILE" ]; then
            echo "üìä Verification metrics for ${{ matrix.config }}:"
            
            # Extract key metrics
            STATES=$(grep -o '[0-9]* distinct states' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
            GENERATED=$(grep -o '[0-9]* states generated' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
            VIOLATIONS=$(grep -c 'Error:' "$LOG_FILE" || echo "0")
            
            echo "  States explored: $STATES"
            echo "  States generated: $GENERATED"
            echo "  Violations found: $VIOLATIONS"
            
            # Save metrics
            cat > "results/ci/metrics_${{ matrix.config }}.json" << EOF
          {
            "config": "${{ matrix.config }}",
            "timestamp": "$(date -Iseconds)",
            "states_explored": $STATES,
            "states_generated": $GENERATED,
            "violations": $VIOLATIONS,
            "timeout": ${timeout:-false}
          }
          EOF
          fi

      - name: Upload model checking results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: model-checking-${{ matrix.config }}
          path: |
            results/ci/model_${{ matrix.config }}.log
            results/ci/metrics_${{ matrix.config }}.json
          retention-days: 30

  tlaps-verification:
    name: TLAPS Proof Verification
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    timeout-minutes: 120
    strategy:
      matrix:
        module: [Types, Utils, Safety, Liveness, Resilience, WhitepaperTheorems, Sampling]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ocaml opam z3 cvc4

      - name: Cache TLAPS
        uses: actions/cache@v4
        with:
          path: |
            ~/.opam
            /usr/local/tlaps
          key: tlaps-${{ env.TLAPS_VERSION }}-${{ runner.os }}-v2

      - name: Install TLAPS
        run: |
          if [ ! -f "/usr/local/tlaps/bin/tlapm" ]; then
            echo "üì• Installing TLAPS..."
            
            # Initialize opam if needed
            if [ ! -d ~/.opam ]; then
              opam init --disable-sandboxing -y
            fi
            
            eval $(opam env)
            
            # Install TLAPS dependencies
            opam install -y zarith
            
            # Download and install TLAPS
            wget -q "https://github.com/tlaplus/tlapm/releases/download/v${{ env.TLAPS_VERSION }}/tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin"
            chmod +x "tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin"
            sudo "./tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin" -d /usr/local/tlaps
            
            # Verify installation
            /usr/local/tlaps/bin/tlapm --help > /dev/null
            echo "‚úÖ TLAPS installed successfully"
          else
            echo "‚úÖ TLAPS already installed"
          fi

      - name: Run TLAPS verification with dependency ordering
        run: |
          echo "üîç Running TLAPS verification for ${{ matrix.module }}..."
          
          export PATH="/usr/local/tlaps/bin:$PATH"
          mkdir -p results/ci/tlaps
          
          # Define dependency order
          case "${{ matrix.module }}" in
            Types|Utils)
              DEPENDENCIES=""
              ;;
            Safety)
              DEPENDENCIES="Types Utils"
              ;;
            Liveness)
              DEPENDENCIES="Types Utils Safety"
              ;;
            Resilience)
              DEPENDENCIES="Types Utils Safety Liveness"
              ;;
            WhitepaperTheorems)
              DEPENDENCIES="Types Utils Safety Liveness Resilience"
              ;;
          esac
          
          # Check if module file exists
          MODULE_PATH=""
          if [ -f "specs/${{ matrix.module }}.tla" ]; then
            MODULE_PATH="specs/${{ matrix.module }}.tla"
          elif [ -f "proofs/${{ matrix.module }}.tla" ]; then
            MODULE_PATH="proofs/${{ matrix.module }}.tla"
          else
            echo "‚ö†Ô∏è Module ${{ matrix.module }}.tla not found"
            exit 0
          fi
          
          echo "  Module path: $MODULE_PATH"
          echo "  Dependencies: $DEPENDENCIES"
          
          # Run TLAPS with comprehensive verification script
          if [ -f "scripts/ci/verify_proofs.sh" ]; then
            chmod +x scripts/ci/verify_proofs.sh
            
            timeout 3600 ./scripts/ci/verify_proofs.sh \
              --module "${{ matrix.module }}" \
              --verbose \
              --ci \
              > "results/ci/tlaps/tlaps_${{ matrix.module }}.log" 2>&1
          else
            # Fallback to direct TLAPS invocation
            timeout 3600 tlapm \
              --verbose \
              --toolbox 0 0 \
              --threads 4 \
              "$MODULE_PATH" \
              > "results/ci/tlaps/tlaps_${{ matrix.module }}.log" 2>&1
          fi
          
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ TLAPS verification passed for ${{ matrix.module }}"
            STATUS="VERIFIED"
          elif [ $EXIT_CODE -eq 124 ]; then
            echo "‚ö†Ô∏è TLAPS verification timed out for ${{ matrix.module }}"
            STATUS="TIMEOUT"
          else
            echo "‚ö†Ô∏è TLAPS verification incomplete for ${{ matrix.module }}"
            STATUS="PARTIAL"
          fi
          
          # Extract detailed metrics
          LOG_FILE="results/ci/tlaps/tlaps_${{ matrix.module }}.log"
          OBLIGATIONS=$(grep -c 'obligation' "$LOG_FILE" || echo "0")
          VERIFIED=$(grep -c 'proved' "$LOG_FILE" || echo "0")
          FAILED=$(grep -c 'failed' "$LOG_FILE" || echo "0")
          TIMEOUT_COUNT=$(grep -c 'timeout' "$LOG_FILE" || echo "0")
          
          echo "üìä Verification metrics for ${{ matrix.module }}:"
          echo "  Total obligations: $OBLIGATIONS"
          echo "  Verified: $VERIFIED"
          echo "  Failed: $FAILED"
          echo "  Timeouts: $TIMEOUT_COUNT"
          
          # Save detailed metrics
          cat > "results/ci/tlaps/metrics_${{ matrix.module }}.json" << EOF
          {
            "module": "${{ matrix.module }}",
            "timestamp": "$(date -Iseconds)",
            "status": "$STATUS",
            "total_obligations": $OBLIGATIONS,
            "verified_obligations": $VERIFIED,
            "failed_obligations": $FAILED,
            "timeout_obligations": $TIMEOUT_COUNT,
            "verification_rate": $(echo "scale=2; $VERIFIED * 100 / $OBLIGATIONS" | bc -l 2>/dev/null || echo "0"),
            "dependencies": "$DEPENDENCIES"
          }
          EOF

      - name: Upload TLAPS verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tlaps-verification-${{ matrix.module }}
          path: |
            results/ci/tlaps/tlaps_${{ matrix.module }}.log
            results/ci/tlaps/metrics_${{ matrix.module }}.json
          retention-days: 30

  theorem3-verification:
    name: Theorem 3 (Sampling) Verification
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    timeout-minutes: 120
    strategy:
      matrix:
        verification_type: [tlaps-sampling, tlc-sampling, stateright-sampling]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Setup Rust (for Stateright sampling tests)
        if: matrix.verification_type == 'stateright-sampling'
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/tla-tools
            ~/.cargo/registry
            ~/.cargo/git
            stateright/target
          key: theorem3-${{ matrix.verification_type }}-${{ runner.os }}-${{ hashFiles('stateright/Cargo.lock') }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run Theorem 3 TLAPS verification
        if: matrix.verification_type == 'tlaps-sampling'
        run: |
          echo "üîç Running TLAPS verification for Sampling theorem..."
          
          mkdir -p results/ci/theorem3
          
          if [ -f "proofs/Sampling.tla" ]; then
            export PATH="/usr/local/tlaps/bin:$PATH"
            
            timeout 3600 tlapm \
              --verbose \
              --toolbox 0 0 \
              --threads 4 \
              proofs/Sampling.tla \
              > "results/ci/theorem3/sampling_tlaps.log" 2>&1
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "‚úÖ Sampling TLAPS verification passed"
              STATUS="VERIFIED"
            else
              echo "‚ö†Ô∏è Sampling TLAPS verification incomplete"
              STATUS="PARTIAL"
            fi
            
            # Extract sampling-specific metrics
            LOG_FILE="results/ci/theorem3/sampling_tlaps.log"
            SAMPLING_OBLIGATIONS=$(grep -c 'SamplingResilience\|PartitionSampling\|AdversarialSamplingProbability' "$LOG_FILE" || echo "0")
            SAMPLING_VERIFIED=$(grep -c 'proved.*Sampling' "$LOG_FILE" || echo "0")
            
            echo "üìä Sampling verification metrics:"
            echo "  Sampling obligations: $SAMPLING_OBLIGATIONS"
            echo "  Sampling verified: $SAMPLING_VERIFIED"
            
            cat > "results/ci/theorem3/sampling_tlaps_metrics.json" << EOF
            {
              "module": "Sampling",
              "timestamp": "$(date -Iseconds)",
              "status": "$STATUS",
              "sampling_obligations": $SAMPLING_OBLIGATIONS,
              "sampling_verified": $SAMPLING_VERIFIED,
              "verification_type": "tlaps"
            }
            EOF
          else
            echo "‚ö†Ô∏è Sampling.tla not found"
          fi

      - name: Run Theorem 3 TLC verification
        if: matrix.verification_type == 'tlc-sampling'
        run: |
          echo "üîç Running TLC verification for Sampling configuration..."
          
          mkdir -p results/ci/theorem3
          
          if [ -f "models/SamplingVerification.cfg" ]; then
            timeout 2400 java -cp ~/tla-tools/tla2tools.jar tlc2.TLC \
              -config models/SamplingVerification.cfg \
              -workers 4 \
              -verbose \
              specs/Rotor \
              > "results/ci/theorem3/sampling_tlc.log" 2>&1
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "‚úÖ Sampling TLC verification passed"
              STATUS="VERIFIED"
            else
              echo "‚ö†Ô∏è Sampling TLC verification incomplete"
              STATUS="PARTIAL"
            fi
            
            # Extract sampling TLC metrics
            LOG_FILE="results/ci/theorem3/sampling_tlc.log"
            SAMPLING_STATES=$(grep -o '[0-9]* distinct states' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
            SAMPLING_PROPERTIES=$(grep -c 'SamplingResilience\|PartitioningValidity' "$LOG_FILE" || echo "0")
            
            echo "üìä Sampling TLC metrics:"
            echo "  States explored: $SAMPLING_STATES"
            echo "  Sampling properties: $SAMPLING_PROPERTIES"
            
            cat > "results/ci/theorem3/sampling_tlc_metrics.json" << EOF
            {
              "config": "SamplingVerification",
              "timestamp": "$(date -Iseconds)",
              "status": "$STATUS",
              "states_explored": $SAMPLING_STATES,
              "sampling_properties": $SAMPLING_PROPERTIES,
              "verification_type": "tlc"
            }
            EOF
          else
            echo "‚ö†Ô∏è SamplingVerification.cfg not found"
          fi

      - name: Run Theorem 3 Stateright verification
        if: matrix.verification_type == 'stateright-sampling'
        run: |
          echo "üîç Running Stateright verification for Sampling implementation..."
          
          mkdir -p results/ci/theorem3
          
          if [ -d "stateright" ] && [ -f "stateright/tests/sampling_verification.rs" ]; then
            cd stateright
            
            # Build sampling implementation
            cargo build --release --features sampling
            
            # Run sampling-specific tests
            cargo test --release sampling_verification -- \
              --nocapture \
              --test-threads 1 \
              > "../results/ci/theorem3/sampling_stateright.log" 2>&1
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "‚úÖ Sampling Stateright verification passed"
              STATUS="VERIFIED"
            else
              echo "‚ö†Ô∏è Sampling Stateright verification incomplete"
              STATUS="PARTIAL"
            fi
            
            # Extract sampling Stateright metrics
            LOG_FILE="../results/ci/theorem3/sampling_stateright.log"
            SAMPLING_TESTS=$(grep -c 'test.*sampling.*ok' "$LOG_FILE" || echo "0")
            RESILIENCE_TESTS=$(grep -c 'test.*resilience.*ok' "$LOG_FILE" || echo "0")
            
            echo "üìä Sampling Stateright metrics:"
            echo "  Sampling tests passed: $SAMPLING_TESTS"
            echo "  Resilience tests passed: $RESILIENCE_TESTS"
            
            cat > "../results/ci/theorem3/sampling_stateright_metrics.json" << EOF
            {
              "component": "sampling",
              "timestamp": "$(date -Iseconds)",
              "status": "$STATUS",
              "sampling_tests": $SAMPLING_TESTS,
              "resilience_tests": $RESILIENCE_TESTS,
              "verification_type": "stateright"
            }
            EOF
          else
            echo "‚ö†Ô∏è Stateright sampling tests not found"
          fi

      - name: Upload Theorem 3 verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: theorem3-verification-${{ matrix.verification_type }}
          path: |
            results/ci/theorem3/
          retention-days: 30

  whitepaper-validation:
    name: Whitepaper Validation (TLC)
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    timeout-minutes: 90
    strategy:
      matrix:
        config: [WhitepaperValidation, BoundaryConditions, AdversarialScenarios]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run TLC model checking with WhitepaperValidation
        run: |
          echo "üîç Running TLC model checking for ${{ matrix.config }}..."
          
          mkdir -p results/ci/tlc
          
          # Check if configuration file exists
          CONFIG_FILE="models/${{ matrix.config }}.cfg"
          if [ ! -f "$CONFIG_FILE" ]; then
            echo "‚ö†Ô∏è Configuration file $CONFIG_FILE not found"
            exit 0
          fi
          
          # Set timeout based on configuration
          case "${{ matrix.config }}" in
            WhitepaperValidation) TIMEOUT=2400 ;;
            BoundaryConditions) TIMEOUT=1800 ;;
            AdversarialScenarios) TIMEOUT=3600 ;;
            *) TIMEOUT=1800 ;;
          esac
          
          # Run TLC with comprehensive verification script
          if [ -f "scripts/ci/check_model.sh" ]; then
            chmod +x scripts/ci/check_model.sh
            
            timeout $TIMEOUT ./scripts/ci/check_model.sh \
              --tlc-only \
              --config "${{ matrix.config }}" \
              --verbose \
              --ci \
              > "results/ci/tlc/tlc_${{ matrix.config }}.log" 2>&1
          else
            # Fallback to direct TLC invocation
            timeout $TIMEOUT java -cp ~/tla-tools/tla2tools.jar tlc2.TLC \
              -config "$CONFIG_FILE" \
              -workers 4 \
              -verbose \
              specs/Alpenglow \
              > "results/ci/tlc/tlc_${{ matrix.config }}.log" 2>&1
          fi
          
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ TLC model checking passed for ${{ matrix.config }}"
            STATUS="VERIFIED"
          elif [ $EXIT_CODE -eq 124 ]; then
            echo "‚ö†Ô∏è TLC model checking timed out for ${{ matrix.config }}"
            STATUS="TIMEOUT"
          else
            echo "‚ùå TLC model checking failed for ${{ matrix.config }}"
            STATUS="FAILED"
          fi
          
          # Extract detailed metrics
          LOG_FILE="results/ci/tlc/tlc_${{ matrix.config }}.log"
          STATES_EXPLORED=$(grep -o '[0-9]* distinct states' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
          STATES_GENERATED=$(grep -o '[0-9]* states generated' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
          VIOLATIONS=$(grep -c 'Error:' "$LOG_FILE" || echo "0")
          PROPERTIES_CHECKED=$(grep -c 'Property.*satisfied' "$LOG_FILE" || echo "0")
          
          echo "üìä Model checking metrics for ${{ matrix.config }}:"
          echo "  States explored: $STATES_EXPLORED"
          echo "  States generated: $STATES_GENERATED"
          echo "  Properties checked: $PROPERTIES_CHECKED"
          echo "  Violations found: $VIOLATIONS"
          
          # Save detailed metrics
          cat > "results/ci/tlc/metrics_${{ matrix.config }}.json" << EOF
          {
            "config": "${{ matrix.config }}",
            "timestamp": "$(date -Iseconds)",
            "status": "$STATUS",
            "states_explored": $STATES_EXPLORED,
            "states_generated": $STATES_GENERATED,
            "properties_checked": $PROPERTIES_CHECKED,
            "violations_found": $VIOLATIONS,
            "timeout": $([ "$STATUS" = "TIMEOUT" ] && echo "true" || echo "false")
          }
          EOF

      - name: Upload TLC verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tlc-verification-${{ matrix.config }}
          path: |
            results/ci/tlc/tlc_${{ matrix.config }}.log
            results/ci/tlc/metrics_${{ matrix.config }}.json
          retention-days: 30

  stateright-verification:
    name: Stateright Implementation Verification
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    if: needs.setup.outputs.cross-validate == 'true'
    timeout-minutes: 60
    strategy:
      matrix:
        component: [votor, rotor, integration]
        config: [small, medium]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true
          components: rustfmt, clippy

      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            stateright/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('stateright/Cargo.lock') }}-v2

      - name: Setup Java for TLA+ tools
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Build and test Stateright implementation
        run: |
          if [ -d "stateright" ]; then
            echo "ü¶Ä Building Stateright implementation..."
            cd stateright
            
            # Check code formatting
            cargo fmt --check
            
            # Run clippy for linting
            cargo clippy --all-targets --all-features -- -D warnings
            
            # Build with optimizations
            cargo build --release --all-features
            
            # Run unit tests
            cargo test --release --lib
            
            echo "‚úÖ Stateright implementation built and tested successfully"
          else
            echo "‚ö†Ô∏è Stateright directory not found, skipping verification"
            exit 0
          fi

      - name: Run Stateright model checking
        run: |
          if [ -d "stateright" ]; then
            echo "üîÑ Running Stateright model checking for ${{ matrix.component }} (${{ matrix.config }})..."
            
            cd stateright
            mkdir -p ../results/ci/stateright
            
            # Set parameters based on configuration
            case "${{ matrix.config }}" in
              small)
                VALIDATORS=5
                MAX_STEPS=100
                TIMEOUT=900
                ;;
              medium)
                VALIDATORS=7
                MAX_STEPS=200
                TIMEOUT=1800
                ;;
            esac
            
            # Run component-specific verification
            case "${{ matrix.component }}" in
              votor)
                cargo test --release test_votor_properties -- \
                  --validators $VALIDATORS \
                  --max-steps $MAX_STEPS \
                  --timeout $TIMEOUT \
                  > "../results/ci/stateright/stateright_votor_${{ matrix.config }}.log" 2>&1
                ;;
              rotor)
                cargo test --release test_rotor_properties -- \
                  --validators $VALIDATORS \
                  --max-steps $MAX_STEPS \
                  --timeout $TIMEOUT \
                  > "../results/ci/stateright/stateright_rotor_${{ matrix.config }}.log" 2>&1
                ;;
              integration)
                cargo test --release test_integration_properties -- \
                  --validators $VALIDATORS \
                  --max-steps $MAX_STEPS \
                  --timeout $TIMEOUT \
                  > "../results/ci/stateright/stateright_integration_${{ matrix.config }}.log" 2>&1
                ;;
            esac
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "‚úÖ Stateright verification passed for ${{ matrix.component }} (${{ matrix.config }})"
              STATUS="VERIFIED"
            else
              echo "‚ùå Stateright verification failed for ${{ matrix.component }} (${{ matrix.config }})"
              STATUS="FAILED"
            fi
            
            # Extract metrics
            LOG_FILE="../results/ci/stateright/stateright_${{ matrix.component }}_${{ matrix.config }}.log"
            STATES_EXPLORED=$(grep -o 'explored [0-9]* states' "$LOG_FILE" | tail -1 | grep -o '[0-9]*' || echo "0")
            PROPERTIES_CHECKED=$(grep -c 'property.*verified' "$LOG_FILE" || echo "0")
            VIOLATIONS=$(grep -c 'property.*violated' "$LOG_FILE" || echo "0")
            
            echo "üìä Stateright metrics for ${{ matrix.component }} (${{ matrix.config }}):"
            echo "  States explored: $STATES_EXPLORED"
            echo "  Properties checked: $PROPERTIES_CHECKED"
            echo "  Violations found: $VIOLATIONS"
            
            # Save metrics
            cat > "../results/ci/stateright/metrics_${{ matrix.component }}_${{ matrix.config }}.json" << EOF
            {
              "component": "${{ matrix.component }}",
              "config": "${{ matrix.config }}",
              "timestamp": "$(date -Iseconds)",
              "status": "$STATUS",
              "states_explored": $STATES_EXPLORED,
              "properties_checked": $PROPERTIES_CHECKED,
              "violations_found": $VIOLATIONS,
              "validators": $VALIDATORS,
              "max_steps": $MAX_STEPS
            }
            EOF
          fi

      - name: Upload Stateright verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: stateright-verification-${{ matrix.component }}-${{ matrix.config }}
          path: |
            results/ci/stateright/stateright_${{ matrix.component }}_${{ matrix.config }}.log
            results/ci/stateright/metrics_${{ matrix.component }}_${{ matrix.config }}.json
          retention-days: 30

  theorem-mapping:
    name: Generate Theorem Mapping Documentation
    runs-on: ubuntu-latest
    needs: [tlaps-verification, theorem3-verification, whitepaper-validation]
    timeout-minutes: 30
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install jinja2 pyyaml dataclasses markdown beautifulsoup4 lxml

      - name: Download verification artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Generate comprehensive theorem mapping
        run: |
          echo "üìã Generating comprehensive theorem mapping..."
          
          mkdir -p results/ci/theorem-mapping
          
          if [ -f "scripts/generate_theorem_mapping.py" ]; then
            python scripts/generate_theorem_mapping.py \
              --whitepaper "Solana Alpenglow White Paper v1.1.md" \
              --specs-dir specs \
              --proofs-dir proofs \
              --output-dir results/ci/theorem-mapping \
              --project-root . \
              --verification-artifacts verification-artifacts \
              --include-sampling \
              --verbose \
              > results/ci/theorem-mapping/generation.log 2>&1
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "‚úÖ Theorem mapping generation completed"
              STATUS="SUCCESS"
            else
              echo "‚ö†Ô∏è Theorem mapping generation completed with warnings"
              STATUS="PARTIAL"
            fi
            
            # Verify generated files
            GENERATED_FILES=0
            for file in theorem_mapping.md theorem_mapping.json theorem_mapping.html verification_coverage.json; do
              if [ -f "results/ci/theorem-mapping/$file" ]; then
                echo "‚úÖ Generated: $file"
                GENERATED_FILES=$((GENERATED_FILES + 1))
              else
                echo "‚ùå Missing: $file"
              fi
            done
            
            echo "üìä Theorem mapping metrics:"
            echo "  Generated files: $GENERATED_FILES/4"
            echo "  Status: $STATUS"
            
            # Create mapping summary
            cat > results/ci/theorem-mapping/mapping_summary.json << EOF
            {
              "timestamp": "$(date -Iseconds)",
              "status": "$STATUS",
              "generated_files": $GENERATED_FILES,
              "total_expected": 4,
              "whitepaper_path": "Solana Alpenglow White Paper v1.1.md",
              "includes_sampling": true
            }
            EOF
          else
            echo "‚ö†Ô∏è Theorem mapping script not found"
            
            # Create placeholder mapping
            cat > results/ci/theorem-mapping/mapping_summary.json << EOF
            {
              "timestamp": "$(date -Iseconds)",
              "status": "UNAVAILABLE",
              "reason": "generate_theorem_mapping.py not found"
            }
            EOF
          fi

      - name: Upload theorem mapping results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: theorem-mapping
          path: |
            results/ci/theorem-mapping/
          retention-days: 90

  cross-validation-comprehensive:
    name: Comprehensive TLA+ ‚Üî Stateright Cross-Validation
    runs-on: ubuntu-latest
    needs: [tlaps-verification, theorem3-verification, whitepaper-validation, stateright-verification]
    if: needs.setup.outputs.cross-validate == 'true'
    timeout-minutes: 60
    strategy:
      matrix:
        validation_type: [behavioral-equivalence, property-preservation, sampling-consistency]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            stateright/target
            ~/tla-tools
          key: cross-validation-${{ runner.os }}-${{ hashFiles('stateright/Cargo.lock') }}

      - name: Download verification artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Install tools
        run: |
          # Install TLA+ tools
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run comprehensive cross-validation by type
        run: |
          echo "üîÑ Running ${{ matrix.validation_type }} cross-validation..."
          
          mkdir -p results/ci/cross-validation
          
          case "${{ matrix.validation_type }}" in
            behavioral-equivalence)
              echo "Testing behavioral equivalence between TLA+ and Stateright..."
              
              if [ -d "stateright" ] && [ -f "stateright/tests/cross_validation_comprehensive.rs" ]; then
                cd stateright
                
                cargo test --release behavioral_equivalence -- \
                  --nocapture \
                  --test-threads 1 \
                  > "../results/ci/cross-validation/behavioral_equivalence.log" 2>&1
                
                EXIT_CODE=$?
                
                if [ $EXIT_CODE -eq 0 ]; then
                  echo "‚úÖ Behavioral equivalence tests passed"
                  STATUS="PASSED"
                else
                  echo "‚ùå Behavioral equivalence tests failed"
                  STATUS="FAILED"
                fi
                
                # Extract behavioral metrics
                LOG_FILE="../results/ci/cross-validation/behavioral_equivalence.log"
                STATE_TRANSITIONS=$(grep -c 'state_transition_match: true' "$LOG_FILE" || echo "0")
                EXECUTION_TRACES=$(grep -c 'execution_trace_match: true' "$LOG_FILE" || echo "0")
                
                cat > "../results/ci/cross-validation/behavioral_metrics.json" << EOF
                {
                  "validation_type": "behavioral-equivalence",
                  "timestamp": "$(date -Iseconds)",
                  "status": "$STATUS",
                  "state_transitions_matched": $STATE_TRANSITIONS,
                  "execution_traces_matched": $EXECUTION_TRACES
                }
                EOF
              else
                echo "‚ö†Ô∏è Behavioral equivalence tests not available"
              fi
              ;;
              
            property-preservation)
              echo "Testing property preservation between TLA+ and Stateright..."
              
              if [ -d "stateright" ] && [ -f "stateright/tests/cross_validation_comprehensive.rs" ]; then
                cd stateright
                
                cargo test --release property_preservation -- \
                  --nocapture \
                  --test-threads 1 \
                  > "../results/ci/cross-validation/property_preservation.log" 2>&1
                
                EXIT_CODE=$?
                
                if [ $EXIT_CODE -eq 0 ]; then
                  echo "‚úÖ Property preservation tests passed"
                  STATUS="PASSED"
                else
                  echo "‚ùå Property preservation tests failed"
                  STATUS="FAILED"
                fi
                
                # Extract property metrics
                LOG_FILE="../results/ci/cross-validation/property_preservation.log"
                SAFETY_PROPERTIES=$(grep -c 'safety_property_preserved: true' "$LOG_FILE" || echo "0")
                LIVENESS_PROPERTIES=$(grep -c 'liveness_property_preserved: true' "$LOG_FILE" || echo "0")
                
                cat > "../results/ci/cross-validation/property_metrics.json" << EOF
                {
                  "validation_type": "property-preservation",
                  "timestamp": "$(date -Iseconds)",
                  "status": "$STATUS",
                  "safety_properties_preserved": $SAFETY_PROPERTIES,
                  "liveness_properties_preserved": $LIVENESS_PROPERTIES
                }
                EOF
              else
                echo "‚ö†Ô∏è Property preservation tests not available"
              fi
              ;;
              
            sampling-consistency)
              echo "Testing sampling algorithm consistency between TLA+ and Stateright..."
              
              if [ -d "stateright" ] && [ -f "stateright/tests/sampling_verification.rs" ]; then
                cd stateright
                
                cargo test --release sampling_consistency -- \
                  --nocapture \
                  --test-threads 1 \
                  > "../results/ci/cross-validation/sampling_consistency.log" 2>&1
                
                EXIT_CODE=$?
                
                if [ $EXIT_CODE -eq 0 ]; then
                  echo "‚úÖ Sampling consistency tests passed"
                  STATUS="PASSED"
                else
                  echo "‚ùå Sampling consistency tests failed"
                  STATUS="FAILED"
                fi
                
                # Extract sampling consistency metrics
                LOG_FILE="../results/ci/cross-validation/sampling_consistency.log"
                SAMPLING_MATCHES=$(grep -c 'sampling_result_match: true' "$LOG_FILE" || echo "0")
                RESILIENCE_MATCHES=$(grep -c 'resilience_property_match: true' "$LOG_FILE" || echo "0")
                
                cat > "../results/ci/cross-validation/sampling_metrics.json" << EOF
                {
                  "validation_type": "sampling-consistency",
                  "timestamp": "$(date -Iseconds)",
                  "status": "$STATUS",
                  "sampling_results_matched": $SAMPLING_MATCHES,
                  "resilience_properties_matched": $RESILIENCE_MATCHES
                }
                EOF
              else
                echo "‚ö†Ô∏è Sampling consistency tests not available"
              fi
              ;;
          esac

      - name: Generate cross-validation report
        run: |
          echo "üìÑ Generating cross-validation report..."
          
          cat > results/ci/cross-validation/report.md << 'EOF'
          # TLA+ ‚Üî Stateright Cross-Validation Report
          
          ## Overview
          
          This report summarizes the cross-validation between TLA+ specifications and Stateright implementation.
          
          ## Verification Results
          
          ### TLA+ Verification Status
          - TLAPS proof verification: See individual module results
          - TLC model checking: See configuration-specific results
          
          ### Stateright Verification Status
          - Component verification: See component-specific results
          - Property verification: See property-specific results
          
          ### Cross-Validation Results
          - Consistency score: See metrics file
          - Trace equivalence: See detailed logs
          - Property correspondence: See property mapping results
          
          ## Recommendations
          
          Based on the cross-validation results:
          1. Review any inconsistencies between TLA+ and Stateright
          2. Update implementation to match specification where needed
          3. Refine specifications based on implementation insights
          4. Enhance cross-validation coverage for edge cases
          
          Generated: $(date)
          EOF

      - name: Upload cross-validation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cross-validation-${{ matrix.validation_type }}
          path: |
            results/ci/cross-validation/
          retention-days: 30

  cross-validation:
    name: TLA+ ‚Üî Stateright Cross-Validation Summary
    runs-on: ubuntu-latest
    needs: [cross-validation-comprehensive]
    if: needs.setup.outputs.cross-validate == 'true'
    timeout-minutes: 15
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download cross-validation artifacts
        uses: actions/download-artifact@v4
        with:
          path: cross-validation-artifacts/
          pattern: cross-validation-*

      - name: Aggregate cross-validation results
        run: |
          echo "üìä Aggregating cross-validation results..."
          
          mkdir -p results/ci/cross-validation
          
          # Initialize counters
          TOTAL_VALIDATIONS=0
          PASSED_VALIDATIONS=0
          
          # Aggregate results from all validation types
          for artifact_dir in cross-validation-artifacts/cross-validation-*/; do
            if [ -d "$artifact_dir" ]; then
              for metrics_file in "$artifact_dir"*_metrics.json; do
                if [ -f "$metrics_file" ]; then
                  STATUS=$(jq -r '.status' "$metrics_file" 2>/dev/null || echo "UNKNOWN")
                  VALIDATION_TYPE=$(jq -r '.validation_type' "$metrics_file" 2>/dev/null || echo "unknown")
                  
                  TOTAL_VALIDATIONS=$((TOTAL_VALIDATIONS + 1))
                  
                  if [ "$STATUS" = "PASSED" ]; then
                    PASSED_VALIDATIONS=$((PASSED_VALIDATIONS + 1))
                    echo "‚úÖ $VALIDATION_TYPE: PASSED"
                  else
                    echo "‚ùå $VALIDATION_TYPE: $STATUS"
                  fi
                fi
              done
            fi
          done
          
          # Calculate overall consistency score
          if [ $TOTAL_VALIDATIONS -gt 0 ]; then
            CONSISTENCY_SCORE=$(echo "scale=2; $PASSED_VALIDATIONS * 100 / $TOTAL_VALIDATIONS" | bc -l)
          else
            CONSISTENCY_SCORE="0.00"
          fi
          
          echo "üìä Cross-validation summary:"
          echo "  Total validations: $TOTAL_VALIDATIONS"
          echo "  Passed validations: $PASSED_VALIDATIONS"
          echo "  Overall consistency: $CONSISTENCY_SCORE%"
          
          # Create comprehensive cross-validation report
          cat > results/ci/cross-validation/cross_validation_summary.json << EOF
          {
            "timestamp": "$(date -Iseconds)",
            "total_validations": $TOTAL_VALIDATIONS,
            "passed_validations": $PASSED_VALIDATIONS,
            "overall_consistency_score": $CONSISTENCY_SCORE,
            "threshold_met": $(echo "$CONSISTENCY_SCORE >= 80" | bc -l 2>/dev/null || echo "false"),
            "validation_types": [
              "behavioral-equivalence",
              "property-preservation", 
              "sampling-consistency"
            ]
          }
          EOF

      - name: Upload aggregated cross-validation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cross-validation-summary
          path: |
            results/ci/cross-validation/
          retention-days: 30

  benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [setup, model-checking]
    if: needs.setup.outputs.enable-benchmarks == 'true'
    timeout-minutes: 30
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy matplotlib pandas psutil

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run scalability benchmarks
        run: |
          if [ -d "benchmarks" ] && [ -f "benchmarks/scalability.py" ]; then
            echo "üìà Running scalability benchmarks..."
            
            mkdir -p results/ci/benchmarks
            
            python benchmarks/scalability.py \
              --output results/ci/benchmarks/scalability.json \
              --configs Small,Medium \
              --timeout 600
            
            echo "‚úÖ Scalability benchmarks completed"
          else
            echo "‚ö†Ô∏è Scalability benchmarks not available (benchmarks directory removed)"
            mkdir -p results/ci/benchmarks
            echo '{"status": "skipped", "reason": "benchmarks directory not found"}' > results/ci/benchmarks/scalability.json
          fi

      - name: Run performance benchmarks
        run: |
          if [ -d "benchmarks" ] && [ -f "benchmarks/performance.py" ]; then
            echo "‚ö° Running performance benchmarks..."
            
            python benchmarks/performance.py \
              --output results/ci/benchmarks/performance.json \
              --iterations 5
            
            echo "‚úÖ Performance benchmarks completed"
          else
            echo "‚ö†Ô∏è Performance benchmarks not available (benchmarks directory removed)"
            echo '{"status": "skipped", "reason": "benchmarks directory not found"}' > results/ci/benchmarks/performance.json
          fi

      - name: Generate benchmark report
        run: |
          if [ -f "scripts/ci/benchmark_suite.sh" ]; then
            echo "üìä Generating benchmark report..."
            
            chmod +x scripts/ci/benchmark_suite.sh
            ./scripts/ci/benchmark_suite.sh --ci-mode
            
            echo "‚úÖ Benchmark report generated"
          elif [ -f "scripts/dev/benchmark_suite.sh" ]; then
            echo "üìä Generating benchmark report (dev script)..."
            
            chmod +x scripts/dev/benchmark_suite.sh
            ./scripts/dev/benchmark_suite.sh --ci-mode
            
            echo "‚úÖ Benchmark report generated"
          else
            echo "‚ö†Ô∏è Benchmark suite script not available"
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmarks
          path: |
            results/ci/benchmarks/
          retention-days: 30

  nightly-comprehensive:
    name: Nightly Comprehensive Verification
    runs-on: ubuntu-latest
    needs: [setup]
    if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.verification_mode == 'stress')
    timeout-minutes: 240
    strategy:
      matrix:
        verification_scope: [large-scale-model-checking, extended-proof-verification, stress-testing]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup comprehensive verification environment
        run: |
          # Setup for extended verification
          sudo apt-get update
          sudo apt-get install -y htop iotop sysstat
          
          # Increase system limits for large-scale verification
          echo "* soft nofile 65536" | sudo tee -a /etc/security/limits.conf
          echo "* hard nofile 65536" | sudo tee -a /etc/security/limits.conf
          
          # Setup monitoring
          mkdir -p results/ci/nightly
          
          echo "üåô Starting nightly comprehensive verification for ${{ matrix.verification_scope }}"

      - name: Setup Java with increased memory
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Setup Rust for stress testing
        if: matrix.verification_scope == 'stress-testing'
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true

      - name: Cache tools and dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/tla-tools
            ~/.cargo/registry
            ~/.cargo/git
            stateright/target
          key: nightly-${{ matrix.verification_scope }}-${{ runner.os }}-v2

      - name: Install verification tools
        run: |
          # Install TLA+ tools
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run large-scale model checking
        if: matrix.verification_scope == 'large-scale-model-checking'
        run: |
          echo "üîç Running large-scale model checking..."
          
          # Monitor system resources
          (while true; do
            echo "$(date): $(free -h | grep Mem:)" >> results/ci/nightly/memory_usage.log
            sleep 30
          done) &
          MONITOR_PID=$!
          
          # Run large-scale TLC configurations
          LARGE_CONFIGS=("LargeScale" "StressTest" "BoundaryStress")
          
          for config in "${LARGE_CONFIGS[@]}"; do
            if [ -f "models/${config}.cfg" ]; then
              echo "Running large-scale verification for $config..."
              
              timeout 7200 java -Xmx8g -cp ~/tla-tools/tla2tools.jar tlc2.TLC \
                -config "models/${config}.cfg" \
                -workers 8 \
                -verbose \
                specs/Alpenglow \
                > "results/ci/nightly/large_scale_${config}.log" 2>&1 || true
              
              # Extract metrics
              LOG_FILE="results/ci/nightly/large_scale_${config}.log"
              STATES=$(grep -o '[0-9]* distinct states' "$LOG_FILE" | tail -1 | cut -d' ' -f1 || echo "0")
              
              echo "  $config: $STATES states explored"
            fi
          done
          
          # Stop monitoring
          kill $MONITOR_PID 2>/dev/null || true

      - name: Run extended proof verification
        if: matrix.verification_scope == 'extended-proof-verification'
        run: |
          echo "üîç Running extended proof verification..."
          
          # Install TLAPS for extended verification
          sudo apt-get install -y ocaml opam z3 cvc4
          
          if [ ! -f "/usr/local/tlaps/bin/tlapm" ]; then
            opam init --disable-sandboxing -y
            eval $(opam env)
            opam install -y zarith
            
            wget -q "https://github.com/tlaplus/tlapm/releases/download/v${{ env.TLAPS_VERSION }}/tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin"
            chmod +x "tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin"
            sudo "./tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin" -d /usr/local/tlaps
          fi
          
          export PATH="/usr/local/tlaps/bin:$PATH"
          
          # Run extended proof verification with longer timeouts
          PROOF_MODULES=("Safety" "Liveness" "Resilience" "WhitepaperTheorems" "Sampling")
          
          for module in "${PROOF_MODULES[@]}"; do
            if [ -f "proofs/${module}.tla" ]; then
              echo "Extended verification for $module..."
              
              timeout 10800 tlapm \
                --verbose \
                --toolbox 0 0 \
                --threads 8 \
                --timeout 300 \
                "proofs/${module}.tla" \
                > "results/ci/nightly/extended_${module}.log" 2>&1 || true
              
              # Extract detailed metrics
              LOG_FILE="results/ci/nightly/extended_${module}.log"
              OBLIGATIONS=$(grep -c 'obligation' "$LOG_FILE" || echo "0")
              PROVED=$(grep -c 'proved' "$LOG_FILE" || echo "0")
              
              echo "  $module: $PROVED/$OBLIGATIONS obligations proved"
            fi
          done

      - name: Run stress testing
        if: matrix.verification_scope == 'stress-testing'
        run: |
          echo "üîç Running stress testing..."
          
          if [ -d "stateright" ]; then
            cd stateright
            
            # Build with optimizations
            cargo build --release --all-features
            
            # Run stress tests with large configurations
            STRESS_TESTS=("stress_large_network" "stress_byzantine_scenarios" "stress_partition_recovery")
            
            for test in "${STRESS_TESTS[@]}"; do
              echo "Running stress test: $test"
              
              timeout 3600 cargo test --release "$test" -- \
                --validators 20 \
                --max-steps 1000 \
                --byzantine-percentage 19 \
                --nocapture \
                > "../results/ci/nightly/stress_${test}.log" 2>&1 || true
              
              # Extract stress test metrics
              LOG_FILE="../results/ci/nightly/stress_${test}.log"
              SCENARIOS=$(grep -c 'scenario.*completed' "$LOG_FILE" || echo "0")
              
              echo "  $test: $SCENARIOS scenarios completed"
            done
          fi

      - name: Generate nightly verification report
        run: |
          echo "üìä Generating nightly verification report..."
          
          cat > results/ci/nightly/nightly_report.json << EOF
          {
            "timestamp": "$(date -Iseconds)",
            "verification_scope": "${{ matrix.verification_scope }}",
            "duration_minutes": $(( ($(date +%s) - $(date -d "4 hours ago" +%s)) / 60 )),
            "system_resources": {
              "max_memory_used": "$(grep -o '[0-9]*Gi' results/ci/nightly/memory_usage.log | tail -1 || echo '0Gi')",
              "cpu_cores_used": 8
            }
          }
          EOF

      - name: Upload nightly verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: nightly-verification-${{ matrix.verification_scope }}
          path: |
            results/ci/nightly/
          retention-days: 7

  performance-regression:
    name: Performance Regression Testing
    runs-on: ubuntu-latest
    needs: [model-checking, tlaps-verification, stateright-verification]
    timeout-minutes: 45
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python for performance analysis
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install performance analysis tools
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas matplotlib seaborn psutil

      - name: Download verification artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Analyze verification performance
        run: |
          echo "üìà Analyzing verification performance..."
          
          mkdir -p results/ci/performance-regression
          
          # Create performance analysis script
          cat > analyze_performance_regression.py << 'EOF'
          import json
          import os
          import pandas as pd
          from datetime import datetime
          
          def analyze_performance():
              metrics = []
              
              # Collect timing data from all verification artifacts
              for root, dirs, files in os.walk('verification-artifacts'):
                  for file in files:
                      if file.endswith('_metrics.json'):
                          filepath = os.path.join(root, file)
                          try:
                              with open(filepath, 'r') as f:
                                  data = json.load(f)
                                  if 'timestamp' in data:
                                      metrics.append(data)
                          except Exception as e:
                              print(f"Error reading {filepath}: {e}")
              
              if not metrics:
                  print("No performance metrics found")
                  return
              
              # Calculate performance statistics
              total_components = len(metrics)
              avg_duration = sum(m.get('duration_seconds', 0) for m in metrics) / total_components if total_components > 0 else 0
              success_rate = sum(1 for m in metrics if m.get('status') in ['VERIFIED', 'success']) / total_components * 100 if total_components > 0 else 0
              
              # Performance regression analysis
              performance_data = {
                  'timestamp': datetime.now().isoformat(),
                  'total_components': total_components,
                  'average_duration_seconds': avg_duration,
                  'success_rate_percentage': success_rate,
                  'components_analyzed': [m.get('module', m.get('component', 'unknown')) for m in metrics]
              }
              
              # Check for performance regressions
              baseline_file = 'benchmarks/performance_baseline.json'
              if os.path.exists(baseline_file):
                  with open(baseline_file, 'r') as f:
                      baseline = json.load(f)
                  
                  baseline_duration = baseline.get('average_duration_seconds', 0)
                  if baseline_duration > 0:
                      regression_percentage = ((avg_duration - baseline_duration) / baseline_duration) * 100
                      performance_data['regression_percentage'] = regression_percentage
                      performance_data['baseline_duration'] = baseline_duration
                      
                      if regression_percentage > 20:
                          performance_data['regression_detected'] = True
                          print(f"‚ö†Ô∏è Performance regression detected: {regression_percentage:.1f}%")
                      else:
                          performance_data['regression_detected'] = False
                          print(f"‚úÖ No significant regression: {regression_percentage:.1f}%")
                  else:
                      performance_data['regression_detected'] = False
              else:
                  performance_data['regression_detected'] = False
                  print("No baseline performance data available")
              
              # Save performance analysis
              with open('results/ci/performance-regression/performance_analysis.json', 'w') as f:
                  json.dump(performance_data, f, indent=2)
              
              print(f"Performance Analysis Summary:")
              print(f"  Components: {total_components}")
              print(f"  Average duration: {avg_duration:.1f}s")
              print(f"  Success rate: {success_rate:.1f}%")
          
          if __name__ == "__main__":
              analyze_performance()
          EOF
          
          python analyze_performance_regression.py

      - name: Track verification time trends
        run: |
          echo "üìä Tracking verification time trends..."
          
          # Create trend analysis
          if [ -f "results/ci/performance-regression/performance_analysis.json" ]; then
            CURRENT_DURATION=$(jq -r '.average_duration_seconds' results/ci/performance-regression/performance_analysis.json)
            REGRESSION_DETECTED=$(jq -r '.regression_detected' results/ci/performance-regression/performance_analysis.json)
            
            echo "Current average duration: ${CURRENT_DURATION}s"
            echo "Regression detected: $REGRESSION_DETECTED"
            
            # Set environment variables for later steps
            echo "PERFORMANCE_REGRESSION=$REGRESSION_DETECTED" >> $GITHUB_ENV
            echo "CURRENT_DURATION=$CURRENT_DURATION" >> $GITHUB_ENV
          fi

      - name: Upload performance regression results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-regression
          path: |
            results/ci/performance-regression/
          retention-days: 30

  comprehensive-verification:
    name: Comprehensive Verification Suite
    runs-on: ubuntu-latest
    needs: [tlaps-verification, theorem3-verification, whitepaper-validation, stateright-verification, cross-validation, theorem-mapping]
    if: always()
    timeout-minutes: 45
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all verification artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Setup verification environment
        run: |
          # Install required tools for comprehensive verification
          sudo apt-get update
          sudo apt-get install -y jq bc
          
          mkdir -p results/ci/comprehensive

      - name: Run comprehensive verification script
        run: |
          echo "üîó Running comprehensive verification..."
          
          if [ -f "scripts/run_complete_verification.sh" ]; then
            chmod +x scripts/run_complete_verification.sh
            
            MODE="${{ needs.setup.outputs.verification-mode }}"
            
            # Run comprehensive verification with all components including new ones
            ./scripts/run_complete_verification.sh \
              --$MODE \
              --ci \
              --parallel-jobs 4 \
              --timeout 120 \
              --output-formats console,json \
              > results/ci/comprehensive/comprehensive.log 2>&1
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "‚úÖ Comprehensive verification passed"
              OVERALL_STATUS="PASSED"
            else
              echo "‚ö†Ô∏è Comprehensive verification completed with issues"
              OVERALL_STATUS="PARTIAL"
            fi
          else
            echo "‚ö†Ô∏è Comprehensive verification script not available"
            OVERALL_STATUS="UNAVAILABLE"
          fi
          
          echo "OVERALL_STATUS=$OVERALL_STATUS" >> $GITHUB_ENV

      - name: Aggregate verification results
        run: |
          echo "üìä Aggregating verification results..."
          
          # Initialize counters
          TOTAL_COMPONENTS=0
          SUCCESSFUL_COMPONENTS=0
          FAILED_COMPONENTS=0
          PARTIAL_COMPONENTS=0
          
          # Aggregate TLAPS results (including Sampling)
          for artifact in verification-artifacts/tlaps-verification-*/metrics_*.json; do
            if [ -f "$artifact" ]; then
              STATUS=$(jq -r '.status' "$artifact" 2>/dev/null || echo "UNKNOWN")
              MODULE=$(jq -r '.module' "$artifact" 2>/dev/null || echo "unknown")
              TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
              case "$STATUS" in
                VERIFIED) SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1)) ;;
                PARTIAL) PARTIAL_COMPONENTS=$((PARTIAL_COMPONENTS + 1)) ;;
                *) FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1)) ;;
              esac
              echo "  TLAPS $MODULE: $STATUS"
            fi
          done
          
          # Aggregate Theorem 3 results
          for artifact in verification-artifacts/theorem3-verification-*/sampling_*_metrics.json; do
            if [ -f "$artifact" ]; then
              STATUS=$(jq -r '.status' "$artifact" 2>/dev/null || echo "UNKNOWN")
              VERIFICATION_TYPE=$(jq -r '.verification_type' "$artifact" 2>/dev/null || echo "unknown")
              TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
              case "$STATUS" in
                VERIFIED) SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1)) ;;
                PARTIAL) PARTIAL_COMPONENTS=$((PARTIAL_COMPONENTS + 1)) ;;
                *) FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1)) ;;
              esac
              echo "  Theorem 3 ($VERIFICATION_TYPE): $STATUS"
            fi
          done
          
          # Aggregate TLC results
          for artifact in verification-artifacts/tlc-verification-*/metrics_*.json; do
            if [ -f "$artifact" ]; then
              STATUS=$(jq -r '.status' "$artifact" 2>/dev/null || echo "UNKNOWN")
              CONFIG=$(jq -r '.config' "$artifact" 2>/dev/null || echo "unknown")
              TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
              case "$STATUS" in
                VERIFIED) SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1)) ;;
                TIMEOUT) PARTIAL_COMPONENTS=$((PARTIAL_COMPONENTS + 1)) ;;
                *) FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1)) ;;
              esac
              echo "  TLC $CONFIG: $STATUS"
            fi
          done
          
          # Aggregate Stateright results
          for artifact in verification-artifacts/stateright-verification-*/metrics_*.json; do
            if [ -f "$artifact" ]; then
              STATUS=$(jq -r '.status' "$artifact" 2>/dev/null || echo "UNKNOWN")
              COMPONENT=$(jq -r '.component' "$artifact" 2>/dev/null || echo "unknown")
              TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
              case "$STATUS" in
                VERIFIED) SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1)) ;;
                *) FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1)) ;;
              esac
              echo "  Stateright $COMPONENT: $STATUS"
            fi
          done
          
          # Aggregate Cross-validation results
          if [ -f "verification-artifacts/cross-validation-summary/cross_validation_summary.json" ]; then
            CROSS_VAL_STATUS=$(jq -r '.threshold_met' "verification-artifacts/cross-validation-summary/cross_validation_summary.json" 2>/dev/null || echo "false")
            CONSISTENCY_SCORE=$(jq -r '.overall_consistency_score' "verification-artifacts/cross-validation-summary/cross_validation_summary.json" 2>/dev/null || echo "0")
            TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
            if [ "$CROSS_VAL_STATUS" = "true" ]; then
              SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1))
              echo "  Cross-validation: PASSED ($CONSISTENCY_SCORE%)"
            else
              FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1))
              echo "  Cross-validation: FAILED ($CONSISTENCY_SCORE%)"
            fi
          fi
          
          # Aggregate Theorem Mapping results
          if [ -f "verification-artifacts/theorem-mapping/mapping_summary.json" ]; then
            MAPPING_STATUS=$(jq -r '.status' "verification-artifacts/theorem-mapping/mapping_summary.json" 2>/dev/null || echo "UNKNOWN")
            TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
            case "$MAPPING_STATUS" in
              SUCCESS) SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1)) ;;
              PARTIAL) PARTIAL_COMPONENTS=$((PARTIAL_COMPONENTS + 1)) ;;
              *) FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1)) ;;
            esac
            echo "  Theorem Mapping: $MAPPING_STATUS"
          fi
          
          # Calculate success rate
          if [ $TOTAL_COMPONENTS -gt 0 ]; then
            SUCCESS_RATE=$(echo "scale=1; $SUCCESSFUL_COMPONENTS * 100 / $TOTAL_COMPONENTS" | bc -l)
          else
            SUCCESS_RATE="0.0"
          fi
          
          echo "üìä Verification Summary:"
          echo "  Total components: $TOTAL_COMPONENTS"
          echo "  Successful: $SUCCESSFUL_COMPONENTS"
          echo "  Partial: $PARTIAL_COMPONENTS"
          echo "  Failed: $FAILED_COMPONENTS"
          echo "  Success rate: $SUCCESS_RATE%"
          
          # Save aggregated results
          cat > results/ci/comprehensive/aggregated_results.json << EOF
          {
            "timestamp": "$(date -Iseconds)",
            "workflow_run": "${{ github.run_number }}",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "verification_mode": "${{ needs.setup.outputs.verification-mode }}",
            "overall_status": "$OVERALL_STATUS",
            "summary": {
              "total_components": $TOTAL_COMPONENTS,
              "successful_components": $SUCCESSFUL_COMPONENTS,
              "partial_components": $PARTIAL_COMPONENTS,
              "failed_components": $FAILED_COMPONENTS,
              "success_rate": $SUCCESS_RATE
            },
            "component_results": {
              "tlaps_verification": "${{ needs.tlaps-verification.result }}",
              "theorem3_verification": "${{ needs.theorem3-verification.result }}",
              "whitepaper_validation": "${{ needs.whitepaper-validation.result }}",
              "stateright_verification": "${{ needs.stateright-verification.result }}",
              "cross_validation": "${{ needs.cross-validation.result }}",
              "theorem_mapping": "${{ needs.theorem-mapping.result }}",
              "performance_regression": "${{ needs.performance-regression.result }}"
            }
          }
          EOF

      - name: Generate comprehensive verification matrix
        run: |
          echo "üìã Generating comprehensive verification matrix..."
          
          # Read aggregated results
          AGGREGATED_FILE="results/ci/comprehensive/aggregated_results.json"
          
          if [ -f "$AGGREGATED_FILE" ]; then
            OVERALL_STATUS=$(jq -r '.overall_status' "$AGGREGATED_FILE")
            SUCCESS_RATE=$(jq -r '.summary.success_rate' "$AGGREGATED_FILE")
            TOTAL_COMPONENTS=$(jq -r '.summary.total_components' "$AGGREGATED_FILE")
            SUCCESSFUL_COMPONENTS=$(jq -r '.summary.successful_components' "$AGGREGATED_FILE")
          else
            OVERALL_STATUS="UNKNOWN"
            SUCCESS_RATE="0.0"
            TOTAL_COMPONENTS="0"
            SUCCESSFUL_COMPONENTS="0"
          fi
          
          cat > results/ci/comprehensive/verification_matrix.md << EOF
          # Alpenglow Protocol Comprehensive Verification Matrix
          
          **Overall Status**: $OVERALL_STATUS  
          **Success Rate**: $SUCCESS_RATE%  
          **Components Verified**: $SUCCESSFUL_COMPONENTS/$TOTAL_COMPONENTS  
          **Generated**: $(date)
          
          ## Verification Components
          
          | Component | Status | Coverage |
          |-----------|--------|----------|
          | **TLA+ Specifications** | ${{ needs.syntax-check.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} | Syntax validation for all modules |
          | **TLAPS Proof Verification** | ${{ needs.tlaps-verification.result == 'success' && '‚úÖ PASSED' || needs.tlaps-verification.result == 'failure' && '‚ùå FAILED' || '‚ö†Ô∏è PARTIAL' }} | Formal proofs for safety, liveness, resilience |
          | **Theorem 3 (Sampling)** | ${{ needs.theorem3-verification.result == 'success' && '‚úÖ PASSED' || needs.theorem3-verification.result == 'failure' && '‚ùå FAILED' || '‚ö†Ô∏è PARTIAL' }} | PS-P sampling resilience verification |
          | **TLC Model Checking** | ${{ needs.whitepaper-validation.result == 'success' && '‚úÖ PASSED' || needs.whitepaper-validation.result == 'failure' && '‚ùå FAILED' || '‚ö†Ô∏è PARTIAL' }} | Whitepaper validation and boundary conditions |
          | **Stateright Implementation** | ${{ needs.stateright-verification.result == 'success' && '‚úÖ PASSED' || needs.stateright-verification.result == 'failure' && '‚ùå FAILED' || '‚ö†Ô∏è PARTIAL' }} | Rust implementation verification |
          | **Cross-Validation** | ${{ needs.cross-validation.result == 'success' && '‚úÖ PASSED' || needs.cross-validation.result == 'failure' && '‚ùå FAILED' || '‚ö†Ô∏è PARTIAL' }} | TLA+ ‚Üî Stateright consistency |
          | **Theorem Mapping** | ${{ needs.theorem-mapping.result == 'success' && '‚úÖ PASSED' || needs.theorem-mapping.result == 'failure' && '‚ùå FAILED' || '‚ö†Ô∏è PARTIAL' }} | Whitepaper to proof correspondence |
          | **Performance Regression** | ${{ needs.performance-regression.result == 'success' && '‚úÖ PASSED' || needs.performance-regression.result == 'failure' && '‚ùå FAILED' || '‚ö†Ô∏è SKIPPED' }} | Performance monitoring and regression detection |
          | **Performance Benchmarks** | ${{ needs.benchmarks.result == 'success' && '‚úÖ PASSED' || needs.benchmarks.result == 'failure' && '‚ùå FAILED' || '‚ö†Ô∏è SKIPPED' }} | Scalability and performance metrics |
          
          ## Key Properties Verified
          
          ### Safety Properties
          - **No Conflicting Finalizations**: Ensures no two conflicting blocks finalize in the same slot
          - **Certificate Uniqueness**: At most one certificate per slot and type
          - **Chain Consistency**: All honest validators maintain compatible finalized chains
          - **Byzantine Tolerance**: Safety maintained with ‚â§20% Byzantine stake
          
          ### Liveness Properties
          - **Progress Guarantee**: Network continues finalizing blocks with >60% honest stake
          - **Fast Path Liveness**: 100ms finalization with >80% responsive stake
          - **Slow Path Liveness**: 150ms finalization with >60% responsive stake
          - **Bounded Finalization**: Finalization within specified time bounds
          
          ### Resilience Properties
          - **20+20 Resilience**: Safety with ‚â§20% Byzantine + ‚â§20% offline validators
          - **Network Partition Recovery**: Recovery when partitions heal
          - **Attack Resistance**: Resistance to double voting, split voting, withholding attacks
          
          ## Verification Coverage
          
          - **Specifications**: Complete TLA+ formal specifications
          - **Proofs**: Machine-checked TLAPS proofs
          - **Model Checking**: Exhaustive state space exploration
          - **Implementation**: Verified Rust implementation
          - **Cross-Validation**: Consistency between specification and implementation
          - **Performance**: Scalability and timing analysis
          
          ## Artifacts
          
          - Detailed logs available in workflow artifacts
          - Verification metrics in JSON format
          - Cross-validation reports and traces
          - Performance benchmark results
          
          EOF

      - name: Upload comprehensive verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: comprehensive-verification
          path: |
            results/ci/comprehensive/
          retention-days: 90

  performance-monitoring:
    name: Performance Monitoring & Regression Detection
    runs-on: ubuntu-latest
    needs: [comprehensive-verification]
    if: needs.setup.outputs.enable-benchmarks == 'true'
    timeout-minutes: 30
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python for performance analysis
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install analysis dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas matplotlib seaborn jq

      - name: Download verification artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Analyze performance metrics
        run: |
          echo "üìà Analyzing performance metrics..."
          
          mkdir -p results/ci/performance
          
          # Create performance analysis script
          cat > analyze_performance.py << 'EOF'
          import json
          import pandas as pd
          import matplotlib.pyplot as plt
          import os
          from pathlib import Path
          
          def analyze_verification_performance():
              metrics = []
              
              # Collect all metrics files
              for root, dirs, files in os.walk('verification-artifacts'):
                  for file in files:
                      if file.startswith('metrics_') and file.endswith('.json'):
                          filepath = os.path.join(root, file)
                          try:
                              with open(filepath, 'r') as f:
                                  data = json.load(f)
                                  metrics.append(data)
                          except Exception as e:
                              print(f"Error reading {filepath}: {e}")
              
              if not metrics:
                  print("No metrics found")
                  return
              
              # Create DataFrame
              df = pd.DataFrame(metrics)
              
              # Performance summary
              summary = {
                  'total_components': len(df),
                  'avg_verification_time': df.get('execution_time_seconds', pd.Series([0])).mean(),
                  'success_rate': (df['status'] == 'VERIFIED').mean() * 100,
                  'timeout_rate': (df['status'] == 'TIMEOUT').mean() * 100
              }
              
              # Save summary
              with open('results/ci/performance/performance_summary.json', 'w') as f:
                  json.dump(summary, f, indent=2)
              
              print(f"Performance Summary:")
              print(f"  Components analyzed: {summary['total_components']}")
              print(f"  Average verification time: {summary['avg_verification_time']:.1f}s")
              print(f"  Success rate: {summary['success_rate']:.1f}%")
              print(f"  Timeout rate: {summary['timeout_rate']:.1f}%")
          
          if __name__ == "__main__":
              analyze_verification_performance()
          EOF
          
          python analyze_performance.py

      - name: Check for performance regressions
        run: |
          echo "üîç Checking for performance regressions..."
          
          # Compare with previous runs (if available)
          if [ -d "benchmarks" ] && [ -f "benchmarks/baseline_performance.json" ]; then
            echo "Comparing with baseline performance..."
            
            CURRENT_TIME=$(jq -r '.avg_verification_time' results/ci/performance/performance_summary.json 2>/dev/null || echo "0")
            BASELINE_TIME=$(jq -r '.avg_verification_time' benchmarks/baseline_performance.json 2>/dev/null || echo "0")
            
            if [ "$CURRENT_TIME" != "0" ] && [ "$BASELINE_TIME" != "0" ]; then
              REGRESSION=$(echo "scale=2; ($CURRENT_TIME - $BASELINE_TIME) / $BASELINE_TIME * 100" | bc -l 2>/dev/null || echo "0")
              
              echo "Performance comparison:"
              echo "  Baseline time: ${BASELINE_TIME}s"
              echo "  Current time: ${CURRENT_TIME}s"
              echo "  Change: ${REGRESSION}%"
              
              # Flag significant regressions
              if (( $(echo "$REGRESSION > 20" | bc -l 2>/dev/null || echo "0") )); then
                echo "‚ö†Ô∏è Significant performance regression detected: ${REGRESSION}%"
                echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
              else
                echo "‚úÖ No significant performance regression"
                echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
              fi
            fi
          else
            echo "No baseline performance data available (benchmarks directory removed or baseline missing)"
            echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          fi

      - name: Upload performance analysis
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-monitoring
          path: |
            results/ci/performance/
          retention-days: 30

  final-report:
    name: Generate Final Verification Report
    runs-on: ubuntu-latest
    needs: [setup, syntax-check, tlaps-verification, whitepaper-validation, stateright-verification, cross-validation, comprehensive-verification, performance-monitoring]
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Generate comprehensive final report
        run: |
          echo "üìÑ Generating comprehensive final verification report..."
          
          mkdir -p reports/ci
          
          # Collect results from all jobs
          TIMESTAMP=$(date -Iseconds)
          
          cat > reports/ci/final_verification_report.json << EOF
          {
            "metadata": {
              "timestamp": "$TIMESTAMP",
              "workflow_run": "${{ github.run_number }}",
              "commit": "${{ github.sha }}",
              "branch": "${{ github.ref_name }}",
              "verification_mode": "${{ needs.setup.outputs.verification-mode }}",
              "cross_validate": "${{ needs.setup.outputs.cross-validate }}",
              "enable_benchmarks": "${{ needs.setup.outputs.enable-benchmarks }}"
            },
            "job_results": {
              "syntax_check": "${{ needs.syntax-check.result }}",
              "tlaps_verification": "${{ needs.tlaps-verification.result }}",
              "whitepaper_validation": "${{ needs.whitepaper-validation.result }}",
              "stateright_verification": "${{ needs.stateright-verification.result }}",
              "cross_validation": "${{ needs.cross-validation.result }}",
              "comprehensive_verification": "${{ needs.comprehensive-verification.result }}",
              "performance_monitoring": "${{ needs.performance-monitoring.result }}"
            },
            "overall_assessment": {
              "status": "$([ "${{ needs.comprehensive-verification.result }}" = "success" ] && echo "COMPREHENSIVE_PASS" || echo "PARTIAL_VERIFICATION")",
              "confidence_level": "$([ "${{ needs.tlaps-verification.result }}" = "success" ] && [ "${{ needs.cross-validation.result }}" = "success" ] && echo "HIGH" || echo "MEDIUM")",
              "whitepaper_theorems_verified": "${{ needs.tlaps-verification.result == 'success' && 'true' || 'false' }}",
              "implementation_validated": "${{ needs.stateright-verification.result == 'success' && 'true' || 'false' }}",
              "cross_validation_passed": "${{ needs.cross-validation.result == 'success' && 'true' || 'false' }}"
            }
          }
          EOF
          
          echo "üìä Final Verification Report:"
          cat reports/ci/final_verification_report.json | jq .

      - name: Create verification status badge data
        run: |
          # Generate badge data for README
          OVERALL_STATUS=$(jq -r '.overall_assessment.status' reports/ci/final_verification_report.json)
          CONFIDENCE=$(jq -r '.overall_assessment.confidence_level' reports/ci/final_verification_report.json)
          
          case "$OVERALL_STATUS" in
            COMPREHENSIVE_PASS)
              BADGE_COLOR="brightgreen"
              BADGE_MESSAGE="verified"
              ;;
            PARTIAL_VERIFICATION)
              BADGE_COLOR="yellow"
              BADGE_MESSAGE="partial"
              ;;
            *)
              BADGE_COLOR="red"
              BADGE_MESSAGE="failed"
              ;;
          esac
          
          echo "BADGE_COLOR=$BADGE_COLOR" >> $GITHUB_ENV
          echo "BADGE_MESSAGE=$BADGE_MESSAGE" >> $GITHUB_ENV
          echo "CONFIDENCE_LEVEL=$CONFIDENCE" >> $GITHUB_ENV

      - name: Upload final verification report
        uses: actions/upload-artifact@v4
        with:
          name: final-verification-report
          path: |
            reports/ci/
            verification-artifacts/
          retention-days: 180

      - name: Comment on PR with comprehensive results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read final verification report
            const report = JSON.parse(fs.readFileSync('reports/ci/final_verification_report.json', 'utf8'));
            
            const statusIcon = (status) => {
              switch(status) {
                case 'success': return '‚úÖ';
                case 'failure': return '‚ùå';
                case 'cancelled': return '‚èπÔ∏è';
                default: return '‚ö†Ô∏è';
              }
            };
            
            const overallIcon = report.overall_assessment.status === 'COMPREHENSIVE_PASS' ? '‚úÖ' : '‚ö†Ô∏è';
            
            const comment = `## üîí Alpenglow Protocol Verification Results
            
            **Overall Status**: ${overallIcon} ${report.overall_assessment.status}  
            **Confidence Level**: ${report.overall_assessment.confidence_level}  
            **Verification Mode**: ${report.metadata.verification_mode}
            
            ### Verification Components
            
            | Component | Status | Description |
            |-----------|--------|-------------|
            | **Syntax Validation** | ${statusIcon(report.job_results.syntax_check)} | TLA+ specification syntax |
            | **TLAPS Proofs** | ${statusIcon(report.job_results.tlaps_verification)} | Formal theorem verification |
            | **TLC Model Checking** | ${statusIcon(report.job_results.whitepaper_validation)} | Whitepaper validation |
            | **Stateright Implementation** | ${statusIcon(report.job_results.stateright_verification)} | Rust implementation verification |
            | **Cross-Validation** | ${statusIcon(report.job_results.cross_validation)} | TLA+ ‚Üî Stateright consistency |
            | **Performance Monitoring** | ${statusIcon(report.job_results.performance_monitoring)} | Regression detection |
            
            ### Key Achievements
            
            - **Whitepaper Theorems**: ${report.overall_assessment.whitepaper_theorems_verified === 'true' ? '‚úÖ Verified' : '‚ùå Not verified'}
            - **Theorem 3 (Sampling)**: ${report.component_results.theorem3_verification === 'success' ? '‚úÖ Verified' : '‚ùå Not verified'}
            - **Implementation Validation**: ${report.overall_assessment.implementation_validated === 'true' ? '‚úÖ Validated' : '‚ùå Not validated'}
            - **Cross-Validation**: ${report.overall_assessment.cross_validation_passed === 'true' ? '‚úÖ Passed' : '‚ùå Failed'}
            - **Theorem Mapping**: ${report.component_results.theorem_mapping === 'success' ? '‚úÖ Generated' : '‚ùå Failed'}
            - **Performance Regression**: ${report.component_results.performance_regression === 'success' ? '‚úÖ No regression' : '‚ö†Ô∏è Regression detected'}
            
            ### Artifacts
            
            - üìä [Detailed verification metrics](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - üìã [Comprehensive verification matrix](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - üîÑ [Cross-validation reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - üìà [Performance analysis](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            **Timestamp**: ${report.metadata.timestamp}  
            **Workflow Run**: #${report.metadata.workflow_run}  
            **Commit**: \`${report.metadata.commit.substring(0, 8)}\`
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Update verification status
        if: github.ref == 'refs/heads/main'
        run: |
          echo "üìù Updating verification status for main branch..."
          
          # Create or update verification status file
          cat > verification_status.json << EOF
          {
            "last_verification": {
              "timestamp": "$(date -Iseconds)",
              "commit": "${{ github.sha }}",
              "workflow_run": "${{ github.run_number }}",
              "status": "$(jq -r '.overall_assessment.status' reports/ci/final_verification_report.json)",
              "confidence": "$(jq -r '.overall_assessment.confidence_level' reports/ci/final_verification_report.json)"
            },
            "verification_history": {
              "total_runs": "${{ github.run_number }}",
              "last_success": "$([ "$(jq -r '.overall_assessment.status' reports/ci/final_verification_report.json)" = "COMPREHENSIVE_PASS" ] && date -Iseconds || echo "null")"
            }
          }
          EOF
          
          echo "Verification status updated"

  cleanup:
    name: Cleanup & Notification
    runs-on: ubuntu-latest
    needs: [final-report]
    if: always()
    steps:
      - name: Cleanup temporary files
        run: |
          echo "üßπ Cleaning up temporary files..."
          
          # Clean up any temporary verification files
          find /tmp -name "*.tlacov" -delete 2>/dev/null || true
          find /tmp -name "states" -type d -exec rm -rf {} + 2>/dev/null || true
          find /tmp -name "*.log" -delete 2>/dev/null || true
          
          # Clean up large artifacts if disk space is low
          DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
          if [ "$DISK_USAGE" -gt 80 ]; then
            echo "‚ö†Ô∏è High disk usage ($DISK_USAGE%), cleaning up large files..."
            find /tmp -size +100M -delete 2>/dev/null || true
          fi
          
          echo "‚úÖ Cleanup completed"

      - name: Send notification on failure
        if: failure() && github.ref == 'refs/heads/main'
        uses: actions/github-script@v7
        with:
          script: |
            // Create an issue for verification failures on main branch
            const title = `üö® Verification Failure on Main Branch - Run #${{ github.run_number }}`;
            const body = `
            ## Verification Failure Alert
            
            The comprehensive verification suite has failed on the main branch.
            
            **Details:**
            - **Commit**: ${{ github.sha }}
            - **Workflow Run**: #${{ github.run_number }}
            - **Timestamp**: ${new Date().toISOString()}
            - **Triggered by**: ${{ github.actor }}
            
            **Failed Jobs:**
            ${Object.entries({
              'Syntax Check': '${{ needs.syntax-check.result }}',
              'TLAPS Verification': '${{ needs.tlaps-verification.result }}',
              'Theorem 3 Verification': '${{ needs.theorem3-verification.result }}',
              'Whitepaper Validation': '${{ needs.whitepaper-validation.result }}',
              'Stateright Verification': '${{ needs.stateright-verification.result }}',
              'Cross-Validation': '${{ needs.cross-validation.result }}',
              'Theorem Mapping': '${{ needs.theorem-mapping.result }}',
              'Performance Regression': '${{ needs.performance-regression.result }}',
              'Comprehensive Verification': '${{ needs.comprehensive-verification.result }}'
            }).filter(([name, result]) => result === 'failure').map(([name, result]) => `- ${name}: ‚ùå FAILED`).join('\n')}
            
            **Action Required:**
            1. Review the [workflow logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            2. Identify and fix the verification issues
            3. Ensure all tests pass before merging future changes
            
            **Artifacts:**
            - [Verification artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [Detailed logs and reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            This issue will be automatically closed when verification passes again.
            `;
            
            // Check if there's already an open issue for verification failures
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'verification-failure,automated'
            });
            
            if (issues.data.length === 0) {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['verification-failure', 'automated', 'priority-high']
              });
            } else {
              // Update existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues.data[0].number,
                body: `## New Verification Failure\n\n${body}`
              });
            }

      - name: Close verification failure issues on success
        if: success() && github.ref == 'refs/heads/main'
        uses: actions/github-script@v7
        with:
          script: |
            // Close any open verification failure issues
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'verification-failure,automated'
            });
            
            for (const issue of issues.data) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issue.number,
                body: `‚úÖ Verification is now passing again as of commit ${{ github.sha }} (Run #${{ github.run_number }}). Closing this issue.`
              });
              
              await github.rest.issues.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issue.number,
                state: 'closed'
              });
            }
