name: Alpenglow Protocol Verification Suite

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'specs/**'
      - 'proofs/**'
      - 'stateright/**'
      - 'models/**'
      - 'scripts/**'
      - '.github/workflows/verify_all.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'specs/**'
      - 'proofs/**'
      - 'stateright/**'
      - 'models/**'
      - 'scripts/**'
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      verification_mode:
        description: 'Verification mode'
        required: true
        default: 'full'
        type: choice
        options:
          - quick
          - full
          - stress
      enable_benchmarks:
        description: 'Run performance benchmarks'
        required: false
        default: true
        type: boolean
      cross_validate:
        description: 'Enable Stateright cross-validation'
        required: false
        default: true
        type: boolean

env:
  CARGO_TERM_COLOR: always
  TLA_TOOLS_VERSION: "1.8.0"
  TLAPS_VERSION: "1.4.5"
  RUST_BACKTRACE: 1

jobs:
  setup:
    name: Setup and Validation
    runs-on: ubuntu-latest
    outputs:
      verification-mode: ${{ steps.config.outputs.verification-mode }}
      enable-benchmarks: ${{ steps.config.outputs.enable-benchmarks }}
      cross-validate: ${{ steps.config.outputs.cross-validate }}
      matrix-configs: ${{ steps.config.outputs.matrix-configs }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure verification parameters
        id: config
        run: |
          # Determine verification mode
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            MODE="${{ github.event.inputs.verification_mode }}"
            BENCHMARKS="${{ github.event.inputs.enable_benchmarks }}"
            CROSS_VAL="${{ github.event.inputs.cross_validate }}"
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            MODE="full"
            BENCHMARKS="true"
            CROSS_VAL="true"
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            MODE="quick"
            BENCHMARKS="false"
            CROSS_VAL="true"
          else
            MODE="full"
            BENCHMARKS="true"
            CROSS_VAL="true"
          fi
          
          echo "verification-mode=$MODE" >> $GITHUB_OUTPUT
          echo "enable-benchmarks=$BENCHMARKS" >> $GITHUB_OUTPUT
          echo "cross-validate=$CROSS_VAL" >> $GITHUB_OUTPUT
          
          # Configure matrix based on mode
          if [ "$MODE" = "quick" ]; then
            CONFIGS='["Small"]'
          elif [ "$MODE" = "stress" ]; then
            CONFIGS='["Small", "Medium", "LargeScale", "Adversarial"]'
          else
            CONFIGS='["Small", "Medium", "Boundary", "EdgeCase"]'
          fi
          
          echo "matrix-configs=$CONFIGS" >> $GITHUB_OUTPUT
          
          echo "ðŸ”§ Configuration:"
          echo "  Mode: $MODE"
          echo "  Benchmarks: $BENCHMARKS"
          echo "  Cross-validation: $CROSS_VAL"
          echo "  Configs: $CONFIGS"

      - name: Validate file structure
        run: |
          echo "ðŸ“ Validating project structure..."
          
          # Check required directories
          for dir in specs proofs models scripts; do
            if [ ! -d "$dir" ]; then
              echo "âŒ Missing required directory: $dir"
              exit 1
            fi
          done
          
          # Check key specification files
          for spec in Alpenglow Types Network Votor Rotor; do
            if [ ! -f "specs/$spec.tla" ]; then
              echo "âŒ Missing specification: specs/$spec.tla"
              exit 1
            fi
          done
          
          # Check proof files
          for proof in Safety Liveness; do
            if [ ! -f "proofs/$proof.tla" ]; then
              echo "âŒ Missing proof: proofs/$proof.tla"
              exit 1
            fi
          done
          
          # Check scripts
          for script in run_all.sh check_model.sh verify_proofs.sh; do
            if [ ! -f "scripts/$script" ]; then
              echo "âŒ Missing script: scripts/$script"
              exit 1
            fi
            chmod +x "scripts/$script"
          done
          
          echo "âœ… Project structure validated"

  syntax-check:
    name: TLA+ Syntax Validation
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            echo "ðŸ“¥ Downloading TLA+ tools..."
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi
          
          # Verify installation
          java -cp ~/tla-tools/tla2tools.jar tla2sany.SANY -h > /dev/null
          echo "âœ… TLA+ tools installed successfully"

      - name: Validate TLA+ specifications
        run: |
          echo "ðŸ” Validating TLA+ syntax..."
          
          SPECS=(Alpenglow Types Network Votor Rotor Integration)
          ERRORS=0
          
          for spec in "${SPECS[@]}"; do
            if [ -f "specs/$spec.tla" ]; then
              echo "  Checking $spec.tla..."
              if java -cp ~/tla-tools/tla2tools.jar tla2sany.SANY "specs/$spec.tla"; then
                echo "    âœ… $spec.tla syntax valid"
              else
                echo "    âŒ $spec.tla has syntax errors"
                ERRORS=$((ERRORS + 1))
              fi
            else
              echo "    âš ï¸ $spec.tla not found"
            fi
          done
          
          if [ $ERRORS -gt 0 ]; then
            echo "âŒ $ERRORS specification(s) have syntax errors"
            exit 1
          fi
          
          echo "âœ… All specifications passed syntax validation"

      - name: Validate proof files
        run: |
          echo "ðŸ” Validating proof file syntax..."
          
          PROOFS=(Safety Liveness Resilience)
          ERRORS=0
          
          for proof in "${PROOFS[@]}"; do
            if [ -f "proofs/$proof.tla" ]; then
              echo "  Checking $proof.tla..."
              if java -cp ~/tla-tools/tla2tools.jar tla2sany.SANY "proofs/$proof.tla"; then
                echo "    âœ… $proof.tla syntax valid"
              else
                echo "    âŒ $proof.tla has syntax errors"
                ERRORS=$((ERRORS + 1))
              fi
            else
              echo "    âš ï¸ $proof.tla not found"
            fi
          done
          
          if [ $ERRORS -gt 0 ]; then
            echo "âš ï¸ $ERRORS proof file(s) have syntax errors"
          else
            echo "âœ… All proof files passed syntax validation"
          fi

  model-checking:
    name: Model Checking (${{ matrix.config }})
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    strategy:
      matrix:
        config: ${{ fromJson(needs.setup.outputs.matrix-configs) }}
      fail-fast: false
    timeout-minutes: 60
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run model checking
        run: |
          echo "ðŸ” Running model checking for ${{ matrix.config }} configuration..."
          
          # Create results directory
          mkdir -p results/ci
          
          # Set timeout based on configuration
          case "${{ matrix.config }}" in
            Small) TIMEOUT=600 ;;
            Medium) TIMEOUT=1800 ;;
            LargeScale) TIMEOUT=3600 ;;
            *) TIMEOUT=1200 ;;
          esac
          
          # Run model checking with timeout
          timeout $TIMEOUT ./scripts/check_model.sh "${{ matrix.config }}" \
            > "results/ci/model_${{ matrix.config }}.log" 2>&1
          
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… Model checking passed for ${{ matrix.config }}"
          elif [ $EXIT_CODE -eq 124 ]; then
            echo "âš ï¸ Model checking timed out for ${{ matrix.config }}"
            echo "timeout=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ Model checking failed for ${{ matrix.config }}"
            cat "results/ci/model_${{ matrix.config }}.log"
            exit 1
          fi

      - name: Extract verification metrics
        run: |
          LOG_FILE="results/ci/model_${{ matrix.config }}.log"
          
          if [ -f "$LOG_FILE" ]; then
            echo "ðŸ“Š Verification metrics for ${{ matrix.config }}:"
            
            # Extract key metrics
            STATES=$(grep -o '[0-9]* distinct states' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
            GENERATED=$(grep -o '[0-9]* states generated' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
            VIOLATIONS=$(grep -c 'Error:' "$LOG_FILE" || echo "0")
            
            echo "  States explored: $STATES"
            echo "  States generated: $GENERATED"
            echo "  Violations found: $VIOLATIONS"
            
            # Save metrics
            cat > "results/ci/metrics_${{ matrix.config }}.json" << EOF
          {
            "config": "${{ matrix.config }}",
            "timestamp": "$(date -Iseconds)",
            "states_explored": $STATES,
            "states_generated": $GENERATED,
            "violations": $VIOLATIONS,
            "timeout": ${timeout:-false}
          }
          EOF
          fi

      - name: Upload model checking results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: model-checking-${{ matrix.config }}
          path: |
            results/ci/model_${{ matrix.config }}.log
            results/ci/metrics_${{ matrix.config }}.json
          retention-days: 30

  tlaps-verification:
    name: TLAPS Proof Verification
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    timeout-minutes: 120
    strategy:
      matrix:
        module: [Types, Utils, Safety, Liveness, Resilience, WhitepaperTheorems]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ocaml opam z3 cvc4

      - name: Cache TLAPS
        uses: actions/cache@v4
        with:
          path: |
            ~/.opam
            /usr/local/tlaps
          key: tlaps-${{ env.TLAPS_VERSION }}-${{ runner.os }}-v2

      - name: Install TLAPS
        run: |
          if [ ! -f "/usr/local/tlaps/bin/tlapm" ]; then
            echo "ðŸ“¥ Installing TLAPS..."
            
            # Initialize opam if needed
            if [ ! -d ~/.opam ]; then
              opam init --disable-sandboxing -y
            fi
            
            eval $(opam env)
            
            # Install TLAPS dependencies
            opam install -y zarith
            
            # Download and install TLAPS
            wget -q "https://github.com/tlaplus/tlapm/releases/download/v${{ env.TLAPS_VERSION }}/tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin"
            chmod +x "tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin"
            sudo "./tlaps-${{ env.TLAPS_VERSION }}-x86_64-linux-gnu-inst.bin" -d /usr/local/tlaps
            
            # Verify installation
            /usr/local/tlaps/bin/tlapm --help > /dev/null
            echo "âœ… TLAPS installed successfully"
          else
            echo "âœ… TLAPS already installed"
          fi

      - name: Run TLAPS verification with dependency ordering
        run: |
          echo "ðŸ” Running TLAPS verification for ${{ matrix.module }}..."
          
          export PATH="/usr/local/tlaps/bin:$PATH"
          mkdir -p results/ci/tlaps
          
          # Define dependency order
          case "${{ matrix.module }}" in
            Types|Utils)
              DEPENDENCIES=""
              ;;
            Safety)
              DEPENDENCIES="Types Utils"
              ;;
            Liveness)
              DEPENDENCIES="Types Utils Safety"
              ;;
            Resilience)
              DEPENDENCIES="Types Utils Safety Liveness"
              ;;
            WhitepaperTheorems)
              DEPENDENCIES="Types Utils Safety Liveness Resilience"
              ;;
          esac
          
          # Check if module file exists
          MODULE_PATH=""
          if [ -f "specs/${{ matrix.module }}.tla" ]; then
            MODULE_PATH="specs/${{ matrix.module }}.tla"
          elif [ -f "proofs/${{ matrix.module }}.tla" ]; then
            MODULE_PATH="proofs/${{ matrix.module }}.tla"
          else
            echo "âš ï¸ Module ${{ matrix.module }}.tla not found"
            exit 0
          fi
          
          echo "  Module path: $MODULE_PATH"
          echo "  Dependencies: $DEPENDENCIES"
          
          # Run TLAPS with comprehensive verification script
          if [ -f "scripts/verify_whitepaper_theorems.sh" ]; then
            chmod +x scripts/verify_whitepaper_theorems.sh
            
            timeout 3600 ./scripts/verify_whitepaper_theorems.sh \
              --module "${{ matrix.module }}" \
              --verbose \
              --ci \
              > "results/ci/tlaps/tlaps_${{ matrix.module }}.log" 2>&1
          else
            # Fallback to direct TLAPS invocation
            timeout 3600 tlapm \
              --verbose \
              --toolbox 0 0 \
              --threads 4 \
              "$MODULE_PATH" \
              > "results/ci/tlaps/tlaps_${{ matrix.module }}.log" 2>&1
          fi
          
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… TLAPS verification passed for ${{ matrix.module }}"
            STATUS="VERIFIED"
          elif [ $EXIT_CODE -eq 124 ]; then
            echo "âš ï¸ TLAPS verification timed out for ${{ matrix.module }}"
            STATUS="TIMEOUT"
          else
            echo "âš ï¸ TLAPS verification incomplete for ${{ matrix.module }}"
            STATUS="PARTIAL"
          fi
          
          # Extract detailed metrics
          LOG_FILE="results/ci/tlaps/tlaps_${{ matrix.module }}.log"
          OBLIGATIONS=$(grep -c 'obligation' "$LOG_FILE" || echo "0")
          VERIFIED=$(grep -c 'proved' "$LOG_FILE" || echo "0")
          FAILED=$(grep -c 'failed' "$LOG_FILE" || echo "0")
          TIMEOUT_COUNT=$(grep -c 'timeout' "$LOG_FILE" || echo "0")
          
          echo "ðŸ“Š Verification metrics for ${{ matrix.module }}:"
          echo "  Total obligations: $OBLIGATIONS"
          echo "  Verified: $VERIFIED"
          echo "  Failed: $FAILED"
          echo "  Timeouts: $TIMEOUT_COUNT"
          
          # Save detailed metrics
          cat > "results/ci/tlaps/metrics_${{ matrix.module }}.json" << EOF
          {
            "module": "${{ matrix.module }}",
            "timestamp": "$(date -Iseconds)",
            "status": "$STATUS",
            "total_obligations": $OBLIGATIONS,
            "verified_obligations": $VERIFIED,
            "failed_obligations": $FAILED,
            "timeout_obligations": $TIMEOUT_COUNT,
            "verification_rate": $(echo "scale=2; $VERIFIED * 100 / $OBLIGATIONS" | bc -l 2>/dev/null || echo "0"),
            "dependencies": "$DEPENDENCIES"
          }
          EOF

      - name: Upload TLAPS verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tlaps-verification-${{ matrix.module }}
          path: |
            results/ci/tlaps/tlaps_${{ matrix.module }}.log
            results/ci/tlaps/metrics_${{ matrix.module }}.json
          retention-days: 30

  whitepaper-validation:
    name: Whitepaper Validation (TLC)
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    timeout-minutes: 90
    strategy:
      matrix:
        config: [WhitepaperValidation, BoundaryConditions, AdversarialScenarios]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run TLC model checking with WhitepaperValidation
        run: |
          echo "ðŸ” Running TLC model checking for ${{ matrix.config }}..."
          
          mkdir -p results/ci/tlc
          
          # Check if configuration file exists
          CONFIG_FILE="models/${{ matrix.config }}.cfg"
          if [ ! -f "$CONFIG_FILE" ]; then
            echo "âš ï¸ Configuration file $CONFIG_FILE not found"
            exit 0
          fi
          
          # Set timeout based on configuration
          case "${{ matrix.config }}" in
            WhitepaperValidation) TIMEOUT=2400 ;;
            BoundaryConditions) TIMEOUT=1800 ;;
            AdversarialScenarios) TIMEOUT=3600 ;;
            *) TIMEOUT=1800 ;;
          esac
          
          # Run TLC with comprehensive verification script
          if [ -f "scripts/verify_whitepaper_theorems.sh" ]; then
            chmod +x scripts/verify_whitepaper_theorems.sh
            
            timeout $TIMEOUT ./scripts/verify_whitepaper_theorems.sh \
              --tlc-only \
              --config "${{ matrix.config }}" \
              --verbose \
              --ci \
              > "results/ci/tlc/tlc_${{ matrix.config }}.log" 2>&1
          else
            # Fallback to direct TLC invocation
            timeout $TIMEOUT java -cp ~/tla-tools/tla2tools.jar tlc2.TLC \
              -config "$CONFIG_FILE" \
              -workers 4 \
              -verbose \
              specs/Alpenglow \
              > "results/ci/tlc/tlc_${{ matrix.config }}.log" 2>&1
          fi
          
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… TLC model checking passed for ${{ matrix.config }}"
            STATUS="VERIFIED"
          elif [ $EXIT_CODE -eq 124 ]; then
            echo "âš ï¸ TLC model checking timed out for ${{ matrix.config }}"
            STATUS="TIMEOUT"
          else
            echo "âŒ TLC model checking failed for ${{ matrix.config }}"
            STATUS="FAILED"
          fi
          
          # Extract detailed metrics
          LOG_FILE="results/ci/tlc/tlc_${{ matrix.config }}.log"
          STATES_EXPLORED=$(grep -o '[0-9]* distinct states' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
          STATES_GENERATED=$(grep -o '[0-9]* states generated' "$LOG_FILE" | head -1 | cut -d' ' -f1 || echo "0")
          VIOLATIONS=$(grep -c 'Error:' "$LOG_FILE" || echo "0")
          PROPERTIES_CHECKED=$(grep -c 'Property.*satisfied' "$LOG_FILE" || echo "0")
          
          echo "ðŸ“Š Model checking metrics for ${{ matrix.config }}:"
          echo "  States explored: $STATES_EXPLORED"
          echo "  States generated: $STATES_GENERATED"
          echo "  Properties checked: $PROPERTIES_CHECKED"
          echo "  Violations found: $VIOLATIONS"
          
          # Save detailed metrics
          cat > "results/ci/tlc/metrics_${{ matrix.config }}.json" << EOF
          {
            "config": "${{ matrix.config }}",
            "timestamp": "$(date -Iseconds)",
            "status": "$STATUS",
            "states_explored": $STATES_EXPLORED,
            "states_generated": $STATES_GENERATED,
            "properties_checked": $PROPERTIES_CHECKED,
            "violations_found": $VIOLATIONS,
            "timeout": $([ "$STATUS" = "TIMEOUT" ] && echo "true" || echo "false")
          }
          EOF

      - name: Upload TLC verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tlc-verification-${{ matrix.config }}
          path: |
            results/ci/tlc/tlc_${{ matrix.config }}.log
            results/ci/tlc/metrics_${{ matrix.config }}.json
          retention-days: 30

  stateright-verification:
    name: Stateright Implementation Verification
    runs-on: ubuntu-latest
    needs: [setup, syntax-check]
    if: needs.setup.outputs.cross-validate == 'true'
    timeout-minutes: 60
    strategy:
      matrix:
        component: [votor, rotor, integration]
        config: [small, medium]
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true
          components: rustfmt, clippy

      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            stateright/target
          key: ${{ runner.os }}-cargo-${{ hashFiles('stateright/Cargo.lock') }}-v2

      - name: Setup Java for TLA+ tools
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Build and test Stateright implementation
        run: |
          if [ -d "stateright" ]; then
            echo "ðŸ¦€ Building Stateright implementation..."
            cd stateright
            
            # Check code formatting
            cargo fmt --check
            
            # Run clippy for linting
            cargo clippy --all-targets --all-features -- -D warnings
            
            # Build with optimizations
            cargo build --release --all-features
            
            # Run unit tests
            cargo test --release --lib
            
            echo "âœ… Stateright implementation built and tested successfully"
          else
            echo "âš ï¸ Stateright directory not found, skipping verification"
            exit 0
          fi

      - name: Run Stateright model checking
        run: |
          if [ -d "stateright" ]; then
            echo "ðŸ”„ Running Stateright model checking for ${{ matrix.component }} (${{ matrix.config }})..."
            
            cd stateright
            mkdir -p ../results/ci/stateright
            
            # Set parameters based on configuration
            case "${{ matrix.config }}" in
              small)
                VALIDATORS=5
                MAX_STEPS=100
                TIMEOUT=900
                ;;
              medium)
                VALIDATORS=7
                MAX_STEPS=200
                TIMEOUT=1800
                ;;
            esac
            
            # Run component-specific verification
            case "${{ matrix.component }}" in
              votor)
                cargo test --release test_votor_properties -- \
                  --validators $VALIDATORS \
                  --max-steps $MAX_STEPS \
                  --timeout $TIMEOUT \
                  > "../results/ci/stateright/stateright_votor_${{ matrix.config }}.log" 2>&1
                ;;
              rotor)
                cargo test --release test_rotor_properties -- \
                  --validators $VALIDATORS \
                  --max-steps $MAX_STEPS \
                  --timeout $TIMEOUT \
                  > "../results/ci/stateright/stateright_rotor_${{ matrix.config }}.log" 2>&1
                ;;
              integration)
                cargo test --release test_integration_properties -- \
                  --validators $VALIDATORS \
                  --max-steps $MAX_STEPS \
                  --timeout $TIMEOUT \
                  > "../results/ci/stateright/stateright_integration_${{ matrix.config }}.log" 2>&1
                ;;
            esac
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "âœ… Stateright verification passed for ${{ matrix.component }} (${{ matrix.config }})"
              STATUS="VERIFIED"
            else
              echo "âŒ Stateright verification failed for ${{ matrix.component }} (${{ matrix.config }})"
              STATUS="FAILED"
            fi
            
            # Extract metrics
            LOG_FILE="../results/ci/stateright/stateright_${{ matrix.component }}_${{ matrix.config }}.log"
            STATES_EXPLORED=$(grep -o 'explored [0-9]* states' "$LOG_FILE" | tail -1 | grep -o '[0-9]*' || echo "0")
            PROPERTIES_CHECKED=$(grep -c 'property.*verified' "$LOG_FILE" || echo "0")
            VIOLATIONS=$(grep -c 'property.*violated' "$LOG_FILE" || echo "0")
            
            echo "ðŸ“Š Stateright metrics for ${{ matrix.component }} (${{ matrix.config }}):"
            echo "  States explored: $STATES_EXPLORED"
            echo "  Properties checked: $PROPERTIES_CHECKED"
            echo "  Violations found: $VIOLATIONS"
            
            # Save metrics
            cat > "../results/ci/stateright/metrics_${{ matrix.component }}_${{ matrix.config }}.json" << EOF
            {
              "component": "${{ matrix.component }}",
              "config": "${{ matrix.config }}",
              "timestamp": "$(date -Iseconds)",
              "status": "$STATUS",
              "states_explored": $STATES_EXPLORED,
              "properties_checked": $PROPERTIES_CHECKED,
              "violations_found": $VIOLATIONS,
              "validators": $VALIDATORS,
              "max_steps": $MAX_STEPS
            }
            EOF
          fi

      - name: Upload Stateright verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: stateright-verification-${{ matrix.component }}-${{ matrix.config }}
          path: |
            results/ci/stateright/stateright_${{ matrix.component }}_${{ matrix.config }}.log
            results/ci/stateright/metrics_${{ matrix.component }}_${{ matrix.config }}.json
          retention-days: 30

  cross-validation:
    name: TLA+ â†” Stateright Cross-Validation
    runs-on: ubuntu-latest
    needs: [tlaps-verification, whitepaper-validation, stateright-verification]
    if: needs.setup.outputs.cross-validate == 'true' && (needs.tlaps-verification.result == 'success' || needs.whitepaper-validation.result == 'success')
    timeout-minutes: 45
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            stateright/target
            ~/tla-tools
          key: cross-validation-${{ runner.os }}-${{ hashFiles('stateright/Cargo.lock') }}

      - name: Download verification artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Install tools
        run: |
          # Install TLA+ tools
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run comprehensive cross-validation
        run: |
          echo "ðŸ”„ Running comprehensive cross-validation..."
          
          mkdir -p results/ci/cross-validation
          
          if [ -d "stateright" ] && [ -f "stateright/tests/cross_validation.rs" ]; then
            cd stateright
            
            # Run cross-validation tests
            cargo test --release cross_validation -- \
              --nocapture \
              --test-threads 1 \
              > "../results/ci/cross-validation/cross_validation.log" 2>&1
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "âœ… Cross-validation tests passed"
              STATUS="PASSED"
            else
              echo "âŒ Cross-validation tests failed"
              STATUS="FAILED"
            fi
            
            # Extract consistency metrics
            LOG_FILE="../results/ci/cross-validation/cross_validation.log"
            CONSISTENCY_SCORE=$(grep -o 'consistency_score: [0-9.]*' "$LOG_FILE" | tail -1 | cut -d' ' -f2 || echo "0.0")
            TRACE_MATCHES=$(grep -c 'trace_match: true' "$LOG_FILE" || echo "0")
            PROPERTY_MATCHES=$(grep -c 'property_match: true' "$LOG_FILE" || echo "0")
            
            echo "ðŸ“Š Cross-validation metrics:"
            echo "  Consistency score: $CONSISTENCY_SCORE"
            echo "  Trace matches: $TRACE_MATCHES"
            echo "  Property matches: $PROPERTY_MATCHES"
            
            # Save cross-validation metrics
            cat > "../results/ci/cross-validation/cross_validation_metrics.json" << EOF
            {
              "timestamp": "$(date -Iseconds)",
              "status": "$STATUS",
              "consistency_score": $CONSISTENCY_SCORE,
              "trace_matches": $TRACE_MATCHES,
              "property_matches": $PROPERTY_MATCHES,
              "threshold_met": $(echo "$CONSISTENCY_SCORE >= 0.8" | bc -l 2>/dev/null || echo "false")
            }
            EOF
          else
            echo "âš ï¸ Cross-validation tests not available"
            
            # Create placeholder metrics
            cat > "results/ci/cross-validation/cross_validation_metrics.json" << EOF
            {
              "timestamp": "$(date -Iseconds)",
              "status": "SKIPPED",
              "reason": "Cross-validation tests not implemented"
            }
            EOF
          fi

      - name: Generate cross-validation report
        run: |
          echo "ðŸ“„ Generating cross-validation report..."
          
          cat > results/ci/cross-validation/report.md << 'EOF'
          # TLA+ â†” Stateright Cross-Validation Report
          
          ## Overview
          
          This report summarizes the cross-validation between TLA+ specifications and Stateright implementation.
          
          ## Verification Results
          
          ### TLA+ Verification Status
          - TLAPS proof verification: See individual module results
          - TLC model checking: See configuration-specific results
          
          ### Stateright Verification Status
          - Component verification: See component-specific results
          - Property verification: See property-specific results
          
          ### Cross-Validation Results
          - Consistency score: See metrics file
          - Trace equivalence: See detailed logs
          - Property correspondence: See property mapping results
          
          ## Recommendations
          
          Based on the cross-validation results:
          1. Review any inconsistencies between TLA+ and Stateright
          2. Update implementation to match specification where needed
          3. Refine specifications based on implementation insights
          4. Enhance cross-validation coverage for edge cases
          
          Generated: $(date)
          EOF

      - name: Upload cross-validation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cross-validation
          path: |
            results/ci/cross-validation/
          retention-days: 30

  benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [setup, model-checking]
    if: needs.setup.outputs.enable-benchmarks == 'true'
    timeout-minutes: 30
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy matplotlib pandas psutil

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Cache TLA+ tools
        uses: actions/cache@v4
        with:
          path: ~/tla-tools
          key: tla-tools-${{ env.TLA_TOOLS_VERSION }}

      - name: Install TLA+ tools
        run: |
          mkdir -p ~/tla-tools
          if [ ! -f ~/tla-tools/tla2tools.jar ]; then
            wget -q "https://github.com/tlaplus/tlaplus/releases/download/v${{ env.TLA_TOOLS_VERSION }}/tla2tools.jar" \
              -O ~/tla-tools/tla2tools.jar
          fi

      - name: Run scalability benchmarks
        run: |
          if [ -f "benchmarks/scalability.py" ]; then
            echo "ðŸ“ˆ Running scalability benchmarks..."
            
            mkdir -p results/ci/benchmarks
            
            python benchmarks/scalability.py \
              --output results/ci/benchmarks/scalability.json \
              --configs Small,Medium \
              --timeout 600
            
            echo "âœ… Scalability benchmarks completed"
          else
            echo "âš ï¸ Scalability benchmarks not available"
          fi

      - name: Run performance benchmarks
        run: |
          if [ -f "benchmarks/performance.py" ]; then
            echo "âš¡ Running performance benchmarks..."
            
            python benchmarks/performance.py \
              --output results/ci/benchmarks/performance.json \
              --iterations 5
            
            echo "âœ… Performance benchmarks completed"
          else
            echo "âš ï¸ Performance benchmarks not available"
          fi

      - name: Generate benchmark report
        run: |
          if [ -f "scripts/benchmark_suite.sh" ]; then
            echo "ðŸ“Š Generating benchmark report..."
            
            chmod +x scripts/benchmark_suite.sh
            ./scripts/benchmark_suite.sh --ci-mode
            
            echo "âœ… Benchmark report generated"
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmarks
          path: |
            results/ci/benchmarks/
          retention-days: 30

  comprehensive-verification:
    name: Comprehensive Verification Suite
    runs-on: ubuntu-latest
    needs: [tlaps-verification, whitepaper-validation, stateright-verification, cross-validation]
    if: always()
    timeout-minutes: 30
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all verification artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Setup verification environment
        run: |
          # Install required tools for comprehensive verification
          sudo apt-get update
          sudo apt-get install -y jq bc
          
          mkdir -p results/ci/comprehensive

      - name: Run comprehensive verification script
        run: |
          echo "ðŸ”— Running comprehensive verification..."
          
          if [ -f "scripts/verify_whitepaper_theorems.sh" ]; then
            chmod +x scripts/verify_whitepaper_theorems.sh
            
            MODE="${{ needs.setup.outputs.verification-mode }}"
            
            # Run comprehensive verification with all components
            ./scripts/verify_whitepaper_theorems.sh \
              --$MODE \
              --comprehensive \
              --ci \
              --parallel 4 \
              --timeout-tlaps 3600 \
              --timeout-tlc 1800 \
              --timeout-stateright 900 \
              > results/ci/comprehensive/comprehensive.log 2>&1
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "âœ… Comprehensive verification passed"
              OVERALL_STATUS="PASSED"
            else
              echo "âš ï¸ Comprehensive verification completed with issues"
              OVERALL_STATUS="PARTIAL"
            fi
          else
            echo "âš ï¸ Comprehensive verification script not available"
            OVERALL_STATUS="UNAVAILABLE"
          fi
          
          echo "OVERALL_STATUS=$OVERALL_STATUS" >> $GITHUB_ENV

      - name: Aggregate verification results
        run: |
          echo "ðŸ“Š Aggregating verification results..."
          
          # Initialize counters
          TOTAL_COMPONENTS=0
          SUCCESSFUL_COMPONENTS=0
          FAILED_COMPONENTS=0
          PARTIAL_COMPONENTS=0
          
          # Aggregate TLAPS results
          for artifact in verification-artifacts/tlaps-verification-*/metrics_*.json; do
            if [ -f "$artifact" ]; then
              STATUS=$(jq -r '.status' "$artifact" 2>/dev/null || echo "UNKNOWN")
              TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
              case "$STATUS" in
                VERIFIED) SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1)) ;;
                PARTIAL) PARTIAL_COMPONENTS=$((PARTIAL_COMPONENTS + 1)) ;;
                *) FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1)) ;;
              esac
            fi
          done
          
          # Aggregate TLC results
          for artifact in verification-artifacts/tlc-verification-*/metrics_*.json; do
            if [ -f "$artifact" ]; then
              STATUS=$(jq -r '.status' "$artifact" 2>/dev/null || echo "UNKNOWN")
              TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
              case "$STATUS" in
                VERIFIED) SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1)) ;;
                TIMEOUT) PARTIAL_COMPONENTS=$((PARTIAL_COMPONENTS + 1)) ;;
                *) FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1)) ;;
              esac
            fi
          done
          
          # Aggregate Stateright results
          for artifact in verification-artifacts/stateright-verification-*/metrics_*.json; do
            if [ -f "$artifact" ]; then
              STATUS=$(jq -r '.status' "$artifact" 2>/dev/null || echo "UNKNOWN")
              TOTAL_COMPONENTS=$((TOTAL_COMPONENTS + 1))
              case "$STATUS" in
                VERIFIED) SUCCESSFUL_COMPONENTS=$((SUCCESSFUL_COMPONENTS + 1)) ;;
                *) FAILED_COMPONENTS=$((FAILED_COMPONENTS + 1)) ;;
              esac
            fi
          done
          
          # Calculate success rate
          if [ $TOTAL_COMPONENTS -gt 0 ]; then
            SUCCESS_RATE=$(echo "scale=1; $SUCCESSFUL_COMPONENTS * 100 / $TOTAL_COMPONENTS" | bc -l)
          else
            SUCCESS_RATE="0.0"
          fi
          
          echo "ðŸ“Š Verification Summary:"
          echo "  Total components: $TOTAL_COMPONENTS"
          echo "  Successful: $SUCCESSFUL_COMPONENTS"
          echo "  Partial: $PARTIAL_COMPONENTS"
          echo "  Failed: $FAILED_COMPONENTS"
          echo "  Success rate: $SUCCESS_RATE%"
          
          # Save aggregated results
          cat > results/ci/comprehensive/aggregated_results.json << EOF
          {
            "timestamp": "$(date -Iseconds)",
            "workflow_run": "${{ github.run_number }}",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "verification_mode": "${{ needs.setup.outputs.verification-mode }}",
            "overall_status": "$OVERALL_STATUS",
            "summary": {
              "total_components": $TOTAL_COMPONENTS,
              "successful_components": $SUCCESSFUL_COMPONENTS,
              "partial_components": $PARTIAL_COMPONENTS,
              "failed_components": $FAILED_COMPONENTS,
              "success_rate": $SUCCESS_RATE
            },
            "component_results": {
              "tlaps_verification": "${{ needs.tlaps-verification.result }}",
              "whitepaper_validation": "${{ needs.whitepaper-validation.result }}",
              "stateright_verification": "${{ needs.stateright-verification.result }}",
              "cross_validation": "${{ needs.cross-validation.result }}"
            }
          }
          EOF

      - name: Generate comprehensive verification matrix
        run: |
          echo "ðŸ“‹ Generating comprehensive verification matrix..."
          
          # Read aggregated results
          AGGREGATED_FILE="results/ci/comprehensive/aggregated_results.json"
          
          if [ -f "$AGGREGATED_FILE" ]; then
            OVERALL_STATUS=$(jq -r '.overall_status' "$AGGREGATED_FILE")
            SUCCESS_RATE=$(jq -r '.summary.success_rate' "$AGGREGATED_FILE")
            TOTAL_COMPONENTS=$(jq -r '.summary.total_components' "$AGGREGATED_FILE")
            SUCCESSFUL_COMPONENTS=$(jq -r '.summary.successful_components' "$AGGREGATED_FILE")
          else
            OVERALL_STATUS="UNKNOWN"
            SUCCESS_RATE="0.0"
            TOTAL_COMPONENTS="0"
            SUCCESSFUL_COMPONENTS="0"
          fi
          
          cat > results/ci/comprehensive/verification_matrix.md << EOF
          # Alpenglow Protocol Comprehensive Verification Matrix
          
          **Overall Status**: $OVERALL_STATUS  
          **Success Rate**: $SUCCESS_RATE%  
          **Components Verified**: $SUCCESSFUL_COMPONENTS/$TOTAL_COMPONENTS  
          **Generated**: $(date)
          
          ## Verification Components
          
          | Component | Status | Coverage |
          |-----------|--------|----------|
          | **TLA+ Specifications** | ${{ needs.syntax-check.result == 'success' && 'âœ… PASSED' || 'âŒ FAILED' }} | Syntax validation for all modules |
          | **TLAPS Proof Verification** | ${{ needs.tlaps-verification.result == 'success' && 'âœ… PASSED' || needs.tlaps-verification.result == 'failure' && 'âŒ FAILED' || 'âš ï¸ PARTIAL' }} | Formal proofs for safety, liveness, resilience |
          | **TLC Model Checking** | ${{ needs.whitepaper-validation.result == 'success' && 'âœ… PASSED' || needs.whitepaper-validation.result == 'failure' && 'âŒ FAILED' || 'âš ï¸ PARTIAL' }} | Whitepaper validation and boundary conditions |
          | **Stateright Implementation** | ${{ needs.stateright-verification.result == 'success' && 'âœ… PASSED' || needs.stateright-verification.result == 'failure' && 'âŒ FAILED' || 'âš ï¸ PARTIAL' }} | Rust implementation verification |
          | **Cross-Validation** | ${{ needs.cross-validation.result == 'success' && 'âœ… PASSED' || needs.cross-validation.result == 'failure' && 'âŒ FAILED' || 'âš ï¸ PARTIAL' }} | TLA+ â†” Stateright consistency |
          | **Performance Benchmarks** | ${{ needs.benchmarks.result == 'success' && 'âœ… PASSED' || needs.benchmarks.result == 'failure' && 'âŒ FAILED' || 'âš ï¸ SKIPPED' }} | Scalability and performance metrics |
          
          ## Key Properties Verified
          
          ### Safety Properties
          - **No Conflicting Finalizations**: Ensures no two conflicting blocks finalize in the same slot
          - **Certificate Uniqueness**: At most one certificate per slot and type
          - **Chain Consistency**: All honest validators maintain compatible finalized chains
          - **Byzantine Tolerance**: Safety maintained with â‰¤20% Byzantine stake
          
          ### Liveness Properties
          - **Progress Guarantee**: Network continues finalizing blocks with >60% honest stake
          - **Fast Path Liveness**: 100ms finalization with >80% responsive stake
          - **Slow Path Liveness**: 150ms finalization with >60% responsive stake
          - **Bounded Finalization**: Finalization within specified time bounds
          
          ### Resilience Properties
          - **20+20 Resilience**: Safety with â‰¤20% Byzantine + â‰¤20% offline validators
          - **Network Partition Recovery**: Recovery when partitions heal
          - **Attack Resistance**: Resistance to double voting, split voting, withholding attacks
          
          ## Verification Coverage
          
          - **Specifications**: Complete TLA+ formal specifications
          - **Proofs**: Machine-checked TLAPS proofs
          - **Model Checking**: Exhaustive state space exploration
          - **Implementation**: Verified Rust implementation
          - **Cross-Validation**: Consistency between specification and implementation
          - **Performance**: Scalability and timing analysis
          
          ## Artifacts
          
          - Detailed logs available in workflow artifacts
          - Verification metrics in JSON format
          - Cross-validation reports and traces
          - Performance benchmark results
          
          EOF

      - name: Upload comprehensive verification results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: comprehensive-verification
          path: |
            results/ci/comprehensive/
          retention-days: 90

  performance-monitoring:
    name: Performance Monitoring & Regression Detection
    runs-on: ubuntu-latest
    needs: [comprehensive-verification]
    if: needs.setup.outputs.enable-benchmarks == 'true'
    timeout-minutes: 30
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python for performance analysis
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install analysis dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas matplotlib seaborn jq

      - name: Download verification artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Analyze performance metrics
        run: |
          echo "ðŸ“ˆ Analyzing performance metrics..."
          
          mkdir -p results/ci/performance
          
          # Create performance analysis script
          cat > analyze_performance.py << 'EOF'
          import json
          import pandas as pd
          import matplotlib.pyplot as plt
          import os
          from pathlib import Path
          
          def analyze_verification_performance():
              metrics = []
              
              # Collect all metrics files
              for root, dirs, files in os.walk('verification-artifacts'):
                  for file in files:
                      if file.startswith('metrics_') and file.endswith('.json'):
                          filepath = os.path.join(root, file)
                          try:
                              with open(filepath, 'r') as f:
                                  data = json.load(f)
                                  metrics.append(data)
                          except Exception as e:
                              print(f"Error reading {filepath}: {e}")
              
              if not metrics:
                  print("No metrics found")
                  return
              
              # Create DataFrame
              df = pd.DataFrame(metrics)
              
              # Performance summary
              summary = {
                  'total_components': len(df),
                  'avg_verification_time': df.get('execution_time_seconds', pd.Series([0])).mean(),
                  'success_rate': (df['status'] == 'VERIFIED').mean() * 100,
                  'timeout_rate': (df['status'] == 'TIMEOUT').mean() * 100
              }
              
              # Save summary
              with open('results/ci/performance/performance_summary.json', 'w') as f:
                  json.dump(summary, f, indent=2)
              
              print(f"Performance Summary:")
              print(f"  Components analyzed: {summary['total_components']}")
              print(f"  Average verification time: {summary['avg_verification_time']:.1f}s")
              print(f"  Success rate: {summary['success_rate']:.1f}%")
              print(f"  Timeout rate: {summary['timeout_rate']:.1f}%")
          
          if __name__ == "__main__":
              analyze_verification_performance()
          EOF
          
          python analyze_performance.py

      - name: Check for performance regressions
        run: |
          echo "ðŸ” Checking for performance regressions..."
          
          # Compare with previous runs (if available)
          if [ -f "benchmarks/baseline_performance.json" ]; then
            echo "Comparing with baseline performance..."
            
            CURRENT_TIME=$(jq -r '.avg_verification_time' results/ci/performance/performance_summary.json 2>/dev/null || echo "0")
            BASELINE_TIME=$(jq -r '.avg_verification_time' benchmarks/baseline_performance.json 2>/dev/null || echo "0")
            
            if [ "$CURRENT_TIME" != "0" ] && [ "$BASELINE_TIME" != "0" ]; then
              REGRESSION=$(echo "scale=2; ($CURRENT_TIME - $BASELINE_TIME) / $BASELINE_TIME * 100" | bc -l 2>/dev/null || echo "0")
              
              echo "Performance comparison:"
              echo "  Baseline time: ${BASELINE_TIME}s"
              echo "  Current time: ${CURRENT_TIME}s"
              echo "  Change: ${REGRESSION}%"
              
              # Flag significant regressions
              if (( $(echo "$REGRESSION > 20" | bc -l 2>/dev/null || echo "0") )); then
                echo "âš ï¸ Significant performance regression detected: ${REGRESSION}%"
                echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
              else
                echo "âœ… No significant performance regression"
                echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
              fi
            fi
          else
            echo "No baseline performance data available"
            echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          fi

      - name: Upload performance analysis
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-monitoring
          path: |
            results/ci/performance/
          retention-days: 30

  final-report:
    name: Generate Final Verification Report
    runs-on: ubuntu-latest
    needs: [setup, syntax-check, tlaps-verification, whitepaper-validation, stateright-verification, cross-validation, comprehensive-verification, performance-monitoring]
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: verification-artifacts/

      - name: Generate comprehensive final report
        run: |
          echo "ðŸ“„ Generating comprehensive final verification report..."
          
          mkdir -p reports/ci
          
          # Collect results from all jobs
          TIMESTAMP=$(date -Iseconds)
          
          cat > reports/ci/final_verification_report.json << EOF
          {
            "metadata": {
              "timestamp": "$TIMESTAMP",
              "workflow_run": "${{ github.run_number }}",
              "commit": "${{ github.sha }}",
              "branch": "${{ github.ref_name }}",
              "verification_mode": "${{ needs.setup.outputs.verification-mode }}",
              "cross_validate": "${{ needs.setup.outputs.cross-validate }}",
              "enable_benchmarks": "${{ needs.setup.outputs.enable-benchmarks }}"
            },
            "job_results": {
              "syntax_check": "${{ needs.syntax-check.result }}",
              "tlaps_verification": "${{ needs.tlaps-verification.result }}",
              "whitepaper_validation": "${{ needs.whitepaper-validation.result }}",
              "stateright_verification": "${{ needs.stateright-verification.result }}",
              "cross_validation": "${{ needs.cross-validation.result }}",
              "comprehensive_verification": "${{ needs.comprehensive-verification.result }}",
              "performance_monitoring": "${{ needs.performance-monitoring.result }}"
            },
            "overall_assessment": {
              "status": "$([ "${{ needs.comprehensive-verification.result }}" = "success" ] && echo "COMPREHENSIVE_PASS" || echo "PARTIAL_VERIFICATION")",
              "confidence_level": "$([ "${{ needs.tlaps-verification.result }}" = "success" ] && [ "${{ needs.cross-validation.result }}" = "success" ] && echo "HIGH" || echo "MEDIUM")",
              "whitepaper_theorems_verified": "${{ needs.tlaps-verification.result == 'success' && 'true' || 'false' }}",
              "implementation_validated": "${{ needs.stateright-verification.result == 'success' && 'true' || 'false' }}",
              "cross_validation_passed": "${{ needs.cross-validation.result == 'success' && 'true' || 'false' }}"
            }
          }
          EOF
          
          echo "ðŸ“Š Final Verification Report:"
          cat reports/ci/final_verification_report.json | jq .

      - name: Create verification status badge data
        run: |
          # Generate badge data for README
          OVERALL_STATUS=$(jq -r '.overall_assessment.status' reports/ci/final_verification_report.json)
          CONFIDENCE=$(jq -r '.overall_assessment.confidence_level' reports/ci/final_verification_report.json)
          
          case "$OVERALL_STATUS" in
            COMPREHENSIVE_PASS)
              BADGE_COLOR="brightgreen"
              BADGE_MESSAGE="verified"
              ;;
            PARTIAL_VERIFICATION)
              BADGE_COLOR="yellow"
              BADGE_MESSAGE="partial"
              ;;
            *)
              BADGE_COLOR="red"
              BADGE_MESSAGE="failed"
              ;;
          esac
          
          echo "BADGE_COLOR=$BADGE_COLOR" >> $GITHUB_ENV
          echo "BADGE_MESSAGE=$BADGE_MESSAGE" >> $GITHUB_ENV
          echo "CONFIDENCE_LEVEL=$CONFIDENCE" >> $GITHUB_ENV

      - name: Upload final verification report
        uses: actions/upload-artifact@v4
        with:
          name: final-verification-report
          path: |
            reports/ci/
            verification-artifacts/
          retention-days: 180

      - name: Comment on PR with comprehensive results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read final verification report
            const report = JSON.parse(fs.readFileSync('reports/ci/final_verification_report.json', 'utf8'));
            
            const statusIcon = (status) => {
              switch(status) {
                case 'success': return 'âœ…';
                case 'failure': return 'âŒ';
                case 'cancelled': return 'â¹ï¸';
                default: return 'âš ï¸';
              }
            };
            
            const overallIcon = report.overall_assessment.status === 'COMPREHENSIVE_PASS' ? 'âœ…' : 'âš ï¸';
            
            const comment = `## ðŸ”’ Alpenglow Protocol Verification Results
            
            **Overall Status**: ${overallIcon} ${report.overall_assessment.status}  
            **Confidence Level**: ${report.overall_assessment.confidence_level}  
            **Verification Mode**: ${report.metadata.verification_mode}
            
            ### Verification Components
            
            | Component | Status | Description |
            |-----------|--------|-------------|
            | **Syntax Validation** | ${statusIcon(report.job_results.syntax_check)} | TLA+ specification syntax |
            | **TLAPS Proofs** | ${statusIcon(report.job_results.tlaps_verification)} | Formal theorem verification |
            | **TLC Model Checking** | ${statusIcon(report.job_results.whitepaper_validation)} | Whitepaper validation |
            | **Stateright Implementation** | ${statusIcon(report.job_results.stateright_verification)} | Rust implementation verification |
            | **Cross-Validation** | ${statusIcon(report.job_results.cross_validation)} | TLA+ â†” Stateright consistency |
            | **Performance Monitoring** | ${statusIcon(report.job_results.performance_monitoring)} | Regression detection |
            
            ### Key Achievements
            
            - **Whitepaper Theorems**: ${report.overall_assessment.whitepaper_theorems_verified === 'true' ? 'âœ… Verified' : 'âŒ Not verified'}
            - **Implementation Validation**: ${report.overall_assessment.implementation_validated === 'true' ? 'âœ… Validated' : 'âŒ Not validated'}
            - **Cross-Validation**: ${report.overall_assessment.cross_validation_passed === 'true' ? 'âœ… Passed' : 'âŒ Failed'}
            
            ### Artifacts
            
            - ðŸ“Š [Detailed verification metrics](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - ðŸ“‹ [Comprehensive verification matrix](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - ðŸ”„ [Cross-validation reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - ðŸ“ˆ [Performance analysis](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            **Timestamp**: ${report.metadata.timestamp}  
            **Workflow Run**: #${report.metadata.workflow_run}  
            **Commit**: \`${report.metadata.commit.substring(0, 8)}\`
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Update verification status
        if: github.ref == 'refs/heads/main'
        run: |
          echo "ðŸ“ Updating verification status for main branch..."
          
          # Create or update verification status file
          cat > verification_status.json << EOF
          {
            "last_verification": {
              "timestamp": "$(date -Iseconds)",
              "commit": "${{ github.sha }}",
              "workflow_run": "${{ github.run_number }}",
              "status": "$(jq -r '.overall_assessment.status' reports/ci/final_verification_report.json)",
              "confidence": "$(jq -r '.overall_assessment.confidence_level' reports/ci/final_verification_report.json)"
            },
            "verification_history": {
              "total_runs": "${{ github.run_number }}",
              "last_success": "$([ "$(jq -r '.overall_assessment.status' reports/ci/final_verification_report.json)" = "COMPREHENSIVE_PASS" ] && date -Iseconds || echo "null")"
            }
          }
          EOF
          
          echo "Verification status updated"

  cleanup:
    name: Cleanup & Notification
    runs-on: ubuntu-latest
    needs: [final-report]
    if: always()
    steps:
      - name: Cleanup temporary files
        run: |
          echo "ðŸ§¹ Cleaning up temporary files..."
          
          # Clean up any temporary verification files
          find /tmp -name "*.tlacov" -delete 2>/dev/null || true
          find /tmp -name "states" -type d -exec rm -rf {} + 2>/dev/null || true
          find /tmp -name "*.log" -delete 2>/dev/null || true
          
          # Clean up large artifacts if disk space is low
          DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
          if [ "$DISK_USAGE" -gt 80 ]; then
            echo "âš ï¸ High disk usage ($DISK_USAGE%), cleaning up large files..."
            find /tmp -size +100M -delete 2>/dev/null || true
          fi
          
          echo "âœ… Cleanup completed"

      - name: Send notification on failure
        if: failure() && github.ref == 'refs/heads/main'
        uses: actions/github-script@v7
        with:
          script: |
            // Create an issue for verification failures on main branch
            const title = `ðŸš¨ Verification Failure on Main Branch - Run #${{ github.run_number }}`;
            const body = `
            ## Verification Failure Alert
            
            The comprehensive verification suite has failed on the main branch.
            
            **Details:**
            - **Commit**: ${{ github.sha }}
            - **Workflow Run**: #${{ github.run_number }}
            - **Timestamp**: ${new Date().toISOString()}
            - **Triggered by**: ${{ github.actor }}
            
            **Failed Jobs:**
            ${Object.entries({
              'Syntax Check': '${{ needs.syntax-check.result }}',
              'TLAPS Verification': '${{ needs.tlaps-verification.result }}',
              'Whitepaper Validation': '${{ needs.whitepaper-validation.result }}',
              'Stateright Verification': '${{ needs.stateright-verification.result }}',
              'Cross-Validation': '${{ needs.cross-validation.result }}',
              'Comprehensive Verification': '${{ needs.comprehensive-verification.result }}'
            }).filter(([name, result]) => result === 'failure').map(([name, result]) => `- ${name}: âŒ FAILED`).join('\n')}
            
            **Action Required:**
            1. Review the [workflow logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            2. Identify and fix the verification issues
            3. Ensure all tests pass before merging future changes
            
            **Artifacts:**
            - [Verification artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [Detailed logs and reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            This issue will be automatically closed when verification passes again.
            `;
            
            // Check if there's already an open issue for verification failures
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'verification-failure,automated'
            });
            
            if (issues.data.length === 0) {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['verification-failure', 'automated', 'priority-high']
              });
            } else {
              // Update existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues.data[0].number,
                body: `## New Verification Failure\n\n${body}`
              });
            }

      - name: Close verification failure issues on success
        if: success() && github.ref == 'refs/heads/main'
        uses: actions/github-script@v7
        with:
          script: |
            // Close any open verification failure issues
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'verification-failure,automated'
            });
            
            for (const issue of issues.data) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issue.number,
                body: `âœ… Verification is now passing again as of commit ${{ github.sha }} (Run #${{ github.run_number }}). Closing this issue.`
              });
              
              await github.rest.issues.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issue.number,
                state: 'closed'
              });
            }
